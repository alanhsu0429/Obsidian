# Lasso迴歸 (Lasso Regression)

歡迎來到 Lasso 迴歸的世界！Lasso (Least Absolute Shrinkage and Selection Operator) 迴歸是**線性迴歸**的另一種改進，它透過引入**L1 正則化 (L1 Regularization)** 來解決標準線性迴歸中可能出現的**過擬合 (Overfitting)** 問題，並且具備獨特的**特徵選擇 (Feature Selection)** 能力。

---

## 核心概念

*   **L1 正則化 (L1 Regularization)**：Lasso 迴歸使用的正則化類型。它在損失函數中添加了所有係數絕對值之和的懲罰項。這個懲罰項會促使模型將一些不重要的特徵的係數**完全縮小到零**，從而實現特徵選擇。
*   **損失函數 (Loss Function)**：
    Lasso 迴歸的損失函數：
    $J_{lasso}(\beta) = \sum_{i=1}^{m} (y_i - (\beta_0 + \sum_{j=1}^{n} \beta_j x_{ij}))^2 + \lambda \sum_{j=1}^{n} |\beta_j|$
    其中：
    *   $\beta$ 代表模型的係數向量。
    *   $m$ 是樣本數量，$n$ 是特徵數量。
    *   $y_i$ 是真實值，$\hat{y}_i$ 是預測值。
    *   $\lambda$ (lambda) 是**正則化參數 (Regularization Parameter)**，它控制著懲罰項的強度。$\lambda$ 越大，係數被縮小的程度越大，越多係數會被縮小到零；$\lambda$ 越小，Lasso 迴歸越接近標準線性迴歸。

---

## Lasso 迴歸的優點與缺點

### 優點

*   **特徵選擇能力**：這是 Lasso 迴歸最顯著的優勢。它能夠將不重要特徵的係數縮小到零，從而自動進行特徵選擇，簡化模型，提高模型的可解釋性。
*   **防止過擬合**：與嶺迴歸一樣，Lasso 也能透過縮小係數來降低模型的複雜度，減少過擬合的風險。
*   **處理多重共線性**：在存在多重共線性的情況下，Lasso 會傾向於選擇其中一個相關特徵，並將其他相關特徵的係數縮小到零。

### 缺點

*   **當特徵數量大於樣本數量時，最多只能選擇 $m$ 個特徵**：如果有很多相關的特徵，Lasso 可能只會選擇其中一個。
*   **對於高度相關的特徵，Lasso 會隨機選擇一個**：這可能導致模型的不穩定性，即在不同的數據子集上訓練時，可能會選擇不同的特徵。

---

## 正則化參數 $\lambda$ 的選擇

與嶺迴歸一樣，$\lambda$ 是一個超參數，需要透過交叉驗證 (Cross-validation) 等方法來選擇最佳值。選擇合適的 $\lambda$ 值對於平衡模型的偏差 (Bias) 和方差 (Variance) 至關重要。

---

## 應用場景

*   當數據集中包含大量特徵，且其中許多特徵可能是不相關或冗餘時。
*   需要建立一個更簡潔、可解釋性更強的模型時。
*   在基因組學、高維數據分析等領域，進行特徵篩選。

---

Lasso 迴歸因其獨特的特徵選擇能力，在許多實際應用中都非常有用。它與嶺迴歸各有優勢，有時也會結合兩者形成**彈性網路 (Elastic Net)** 迴歸。接下來，我們將探討基於樹的迴歸模型。請持續關注！
