# SARSA

歡迎來到 SARSA 的世界！SARSA 是一種**無模型 (Model-Free)**、**同策略 (On-Policy)** 的**強化學習**演算法。與 Q-learning 類似，它也學習一個動作價值函數 (Q 函數)，但其更新方式與 Q-learning 有所不同，這使得它成為一種同策略學習方法。

---

## 核心概念

*   **無模型 (Model-Free)**：與 Q-learning 一樣，SARSA 也不需要知道環境的轉移機率和獎勵函數。它透過與環境的互動來學習。
*   **同策略 (On-Policy)**：這是 SARSA 與 Q-learning 的主要區別。SARSA 學習的是**當前策略 (Current Policy)** 的 Q 函數，並且在學習過程中，用於更新 Q 值的下一個動作 $a'$ 是**由當前策略選擇的**。這意味著 SARSA 學習的是代理人實際採取的行為的價值，而不是最佳行為的價值。
*   **Q 函數 (Q-function)**：$Q(s, a)$ 表示在狀態 $s$ 下執行動作 $a$ 所能獲得的**預期累積折扣獎勵**。SARSA 的目標是學習一個準確的 Q 函數，以便在任何狀態下都能選擇能最大化未來獎勵的動作。
*   **Bellman 方程 (Bellman Equation)**：SARSA 的更新規則也基於 Bellman 方程，但其形式略有不同：
    $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)]$
    其中：
    *   $s$ 是當前狀態，$a$ 是在 $s$ 中執行的動作。
    *   $s'$ 是執行 $a$ 後到達的下一個狀態，$a'$ 是在 $s'$ 中**由當前策略選擇的**下一個動作。
    *   $r$ 是執行 $a$ 後獲得的即時獎勵。
    *   $\alpha$ (alpha) 是**學習率 (Learning Rate)**，控制每次更新的步長。
    *   $\gamma$ (gamma) 是**折扣因子 (Discount Factor)**，衡量未來獎勵的重要性。

    注意：與 Q-learning 的 $\max_{a'} Q(s', a')$ 不同，SARSA 使用的是 $Q(s', a')$，其中 $a'$ 是實際選擇的動作。這就是同策略的體現。

---

## SARSA 的工作原理

1.  **初始化 Q 表**：將所有狀態-動作對的 Q 值初始化為任意值（通常為零）。
2.  **觀察初始狀態 $s$**。
3.  **根據當前策略選擇動作 $a$**（例如 $\epsilon$-貪婪策略）。
4.  **重複以下步驟，直到達到終止狀態或最大迭代次數**：
    a.  **執行動作 $a$**：在環境中執行動作 $a$，觀察環境返回的即時獎勵 $r$ 和下一個狀態 $s'$。
    b.  **根據當前策略選擇下一個動作 $a'$**：在新的狀態 $s'$ 中，再次使用當前策略（例如 $\epsilon$-貪婪策略）選擇一個動作 $a'$。
    c.  **更新 Q 值**：使用 Bellman 方程更新 $Q(s, a)$ 的值。
    d.  **更新狀態和動作**：將 $s'$ 設為新的當前狀態 $s$，將 $a'$ 設為新的當前動作 $a$。

---

## 優點與缺點

### 優點

*   **無模型**：不需要知道環境的動態，可以直接從經驗中學習。
*   **同策略**：學習的是代理人實際採取的行為的價值，這在某些情況下（例如，在有懲罰的環境中）可能更安全，因為它會考慮到探索行為的風險。
*   **收斂性**：在滿足一定條件下，SARSA 也可以收斂到最佳 Q 函數。

### 缺點

*   **Q 表維度災難**：與 Q-learning 一樣，對於狀態空間或動作空間非常大的問題，儲存和更新 Q 表會變得非常困難。
*   **收斂速度慢**：對於複雜的環境，SARSA 的收斂速度可能很慢。
*   **無法處理連續狀態/動作空間**：SARSA 傳統上是為離散狀態和動作空間設計的。
*   **探索的風險**：由於是同策略，如果探索策略導致代理人進入危險區域，SARSA 會學習到這些危險行為的低價值，這可能導致它在學習過程中表現不佳。

---

## SARSA 與 Q-learning 的比較

| 特性         | Q-learning                                | SARSA                                     |
| :----------- | :---------------------------------------- | :---------------------------------------- |
| **策略**     | 異策略 (Off-Policy)                       | 同策略 (On-Policy)                        |
| **更新目標** | 學習最佳策略的 Q 值                       | 學習當前策略的 Q 值                       |
| **下一個動作** | 選擇 $s'$ 中 Q 值最大的動作 $a'$ (虛擬) | 選擇 $s'$ 中由當前策略實際採取的動作 $a'$ |
| **安全性**   | 可能會學習到最佳路徑，即使該路徑包含危險的探索步驟 | 更傾向於學習安全的策略，因為它考慮了探索的風險 |

---

## 應用場景

*   **遊戲 AI**：在需要考慮探索風險的環境中。
*   **機器人控制**：在物理世界中，避免機器人採取危險動作。

---

SARSA 是強化學習中一個重要的同策略演算法，理解其與 Q-learning 的區別以及同策略的含義是掌握它的關鍵。接下來，我們將探討深度 Q 網路 (DQN)。請持續關注！
