# 決策樹迴歸 (Decision Tree Regression)

歡迎來到決策樹迴歸的世界！決策樹是一種直觀且強大的非線性模型，它透過一系列簡單的決策規則來預測連續型目標變數。你可以將它想像成一個流程圖，根據數據的特徵逐步做出判斷。

---

## 核心概念

*   **樹狀結構 (Tree Structure)**：決策樹由節點 (Nodes) 和邊 (Edges) 組成。
    *   **根節點 (Root Node)**：樹的頂部，代表整個數據集。
    *   **內部節點 (Internal Node)**：代表一個特徵的測試條件（例如「房屋面積是否大於 100 平方米？」）。
    *   **分支 (Branch)**：測試條件的結果，將數據導向不同的子節點。
    *   **葉節點 (Leaf Node)**：樹的末端，代表最終的預測值（在迴歸樹中是一個數值）。
*   **遞歸二元分割 (Recursive Binary Splitting)**：決策樹的建立過程是透過不斷地將數據集分割成兩個子集來進行的。每次分割的目標是使得分割後的子集內部數據的同質性更高（即目標變數的值更接近）。
*   **分割標準 (Splitting Criterion)**：在迴歸樹中，常用的分割標準是**最小化均方誤差 (Mean Squared Error, MSE)** 或**最小化方差 (Variance Reduction)**。演算法會嘗試所有可能的特徵和分割點，選擇能使子節點的 MSE 或方差最小的分割方式。
*   **預測 (Prediction)**：對於一個新的數據點，它會從根節點開始，根據特徵值沿著樹的分支向下遍歷，直到到達一個葉節點。該葉節點的平均目標變數值就是模型的預測結果。

---

## 決策樹迴歸的建立過程

1.  從根節點開始，考慮所有特徵和所有可能的分割點。
2.  選擇一個特徵和分割點，使得分割後兩個子節點的 MSE 或方差之和最小。
3.  將數據集根據選定的分割點分成兩個子集。
4.  對每個子集遞歸地重複步驟 1-3，直到滿足停止條件（例如，節點中的樣本數小於某個閾值，或者樹的深度達到最大值）。
5.  每個葉節點的預測值是該節點中所有訓練樣本目標變數的平均值。

---

## 優點與缺點

### 優點

*   **易於理解和解釋**：樹狀結構直觀，可以很容易地可視化和解釋決策過程。
*   **無需特徵縮放**：對數據的尺度不敏感，不需要進行特徵標準化或歸一化。
*   **能處理非線性關係**：透過多層次的決策，可以捕捉特徵與目標變數之間的複雜非線性關係。
*   **能處理類別型特徵**：可以直接處理類別型特徵，無需進行獨熱編碼 (One-Hot Encoding)。

### 缺點

*   **容易過擬合 (Overfitting)**：單一決策樹很容易過度擬合訓練數據，尤其是在樹的深度較大時。這會導致模型在未見過的數據上表現不佳。
*   **不穩定性 (Instability)**：數據的微小變化（例如，增加或刪除幾個樣本）可能導致樹的結構發生巨大變化。
*   **局部最優 (Local Optimality)**：決策樹的建立是貪婪的，每次分割都選擇當前最優的，但不保證全局最優。

---

## 防止過擬合的策略

*   **剪枝 (Pruning)**：在樹建立完成後，移除一些不重要的分支，以簡化樹的結構。
*   **限制樹的深度 (Max Depth)**：設定樹的最大深度，防止樹生長過深。
*   **限制葉節點的最小樣本數 (Min Samples Leaf)**：設定葉節點必須包含的最小樣本數，避免生成過於細小的葉節點。

---

## 應用場景

*   客戶行為預測
*   醫療診斷輔助
*   金融風險評估

---

決策樹迴歸提供了一種強大的非線性建模能力，但其過擬合的傾向需要我們特別注意。接下來，我們將探討如何透過集成學習來克服單一決策樹的缺點，例如隨機森林。請持續關注！
