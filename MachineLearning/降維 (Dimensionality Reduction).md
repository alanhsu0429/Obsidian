# 降維 (Dimensionality Reduction)

歡迎來到降維的世界！在機器學習中，數據的維度（即特徵數量）往往非常高。高維數據可能帶來許多挑戰，例如計算成本高、模型訓練時間長、難以可視化，甚至可能導致「維度災難」(Curse of Dimensionality)。降維的目標是減少數據的特徵數量，同時盡可能保留數據中的重要信息。

---

## 核心概念

*   **維度災難 (Curse of Dimensionality)**：在高維空間中，數據點變得非常稀疏，使得許多機器學習演算法的性能下降，並且需要指數級增長的數據量來維持模型的性能。
*   **特徵選擇 (Feature Selection)**：從原始特徵集中選擇一個子集，直接移除不相關或冗餘的特徵。這是一種降維方法。
*   **特徵提取 (Feature Extraction)**：將原始特徵轉換為一個新的、維度更低的特徵集。新的特徵是原始特徵的組合或映射，而不是簡單的子集。
*   **信息保留 (Information Preservation)**：降維的關鍵在於在減少維度的同時，盡可能多地保留數據中對任務有用的信息。

---

## 降維的目標

*   **減少儲存空間和計算成本**：更少的特徵意味著更小的數據集，更快的訓練和預測速度。
*   **去除冗餘和噪聲**：高維數據中可能包含大量冗餘或對模型有害的噪聲特徵。
*   **數據可視化**：將高維數據降至 2D 或 3D，以便於人類理解和可視化數據的結構。
*   **提高模型性能**：減少維度災難的影響，有助於提高某些機器學習模型的泛化能力。

---

## 常見的降維演算法

以下是一些常見的降維演算法，我們將在後續的節點中詳細介紹：

1.  **[[主成分分析 (Principal Component Analysis, PCA)]]**：最廣泛使用的線性降維技術。它透過正交變換將數據投影到一個新的坐標系上，使得新的坐標軸（主成分）能夠最大化數據的方差。
2.  **[[線性判別分析 (Linear Discriminant Analysis, LDA)]]**：與 PCA 類似，但 LDA 是一種有監督的降維方法（需要類別標籤）。它的目標是找到一個投影方向，使得不同類別之間的距離最大化，同時同一類別內部的距離最小化。
3.  **[[t-分佈隨機鄰近嵌入 (t-Distributed Stochastic Neighbor Embedding, t-SNE)]]**：一種非線性的降維技術，特別適用於將高維數據可視化到 2D 或 3D 空間，它擅長保留數據點之間的局部結構。
4.  **[[流形學習 (Manifold Learning)]]**：一類非線性降維方法，假設高維數據實際上嵌入在一個低維的「流形」上。常見演算法包括 Isomap, LLE (Locally Linear Embedding) 等。

---

## 降維的注意事項

*   **信息損失**：降維總是伴隨著一定程度的信息損失。選擇合適的降維方法和目標維度是關鍵。
*   **可解釋性**：經過特徵提取後的新特徵可能不如原始特徵那樣具有直觀的可解釋性。

---

降維是處理複雜數據集的強大工具，能夠幫助我們更好地理解和利用數據。接下來，我們將深入探討各種降維演算法。請持續關注！
