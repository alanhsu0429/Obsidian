# Transformer

歡迎來到 Transformer 的世界！Transformer 是一種**深度學習**模型，它在 2017 年由 Google Brain 團隊在論文《Attention Is All You Need》中提出。它徹底改變了**自然語言處理 (NLP)** 領域，並逐漸擴展到**計算機視覺**等其他領域。Transformer 的核心創新是完全放棄了[[循環神經網路 (Recurrent Neural Networks, RNNs)]]的循環結構，轉而完全依賴**自注意力機制 (Self-Attention Mechanism)** 來捕捉序列中的長距離依賴關係。

---

## 核心概念

*   **自注意力機制 (Self-Attention Mechanism)**：這是 Transformer 的核心。它允許模型在處理序列中的每個元素時，都能夠「關注」序列中的所有其他元素，並根據它們的重要性來加權。這使得模型能夠捕捉到序列中任意兩個位置之間的依賴關係，無論它們距離多遠。
    *   **查詢 (Query, Q)**、**鍵 (Key, K)**、**值 (Value, V)**：自注意力機制將每個輸入元素轉換為三個向量：查詢、鍵和值。查詢用於與所有鍵進行匹配，以計算注意力分數，然後這些分數用於加權所有值，得到最終的輸出。
    *   **縮放點積注意力 (Scaled Dot-Product Attention)**：
        $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
        其中 $d_k$ 是鍵向量的維度，用於縮放點積，防止梯度過大。
*   **多頭注意力機制 (Multi-Head Attention)**：為了讓模型能夠從不同的「角度」或「表示子空間」來關注序列，Transformer 使用了多個並行的自注意力機制（稱為「頭」）。每個頭獨立地學習不同的注意力模式，然後將它們的輸出拼接起來，再經過一個線性變換。
*   **位置編碼 (Positional Encoding)**：由於 Transformer 模型沒有循環結構，它無法像 RNN 那樣自然地捕捉序列中元素的順序信息。因此，Transformer 引入了位置編碼，將每個元素在序列中的絕對或相對位置信息編碼成向量，並將其添加到輸入嵌入中。
*   **編碼器-解碼器架構 (Encoder-Decoder Architecture)**：原始的 Transformer 模型採用了編碼器-解碼器架構，非常適合機器翻譯等序列到序列 (Sequence-to-Sequence) 任務。
    *   **編碼器 (Encoder)**：由多個相同的層堆疊而成，每層包含一個多頭自注意力子層和一個前饋神經網路子層。編碼器負責將輸入序列轉換為一系列上下文向量。
    *   **解碼器 (Decoder)**：也由多個相同的層堆疊而成，每層包含一個多頭自注意力子層、一個編碼器-解碼器注意力子層和一個前饋神經網路子層。解碼器負責根據編碼器的輸出和之前生成的輸出，逐步生成目標序列。
*   **前饋神經網路 (Feed-Forward Network)**：每個編碼器和解碼器層都包含一個簡單的前饋神經網路，它獨立地作用於序列中的每個位置。
*   **殘差連接與層歸一化 (Residual Connections and Layer Normalization)**：為了幫助訓練更深層次的網路，Transformer 廣泛使用了殘差連接和層歸一化。

---

## Transformer 的優點與缺點

### 優點

*   **捕捉長距離依賴關係**：自注意力機制能夠直接捕捉序列中任意兩個位置之間的依賴關係，有效解決了 RNN 的長距離依賴問題。
*   **並行化能力強**：由於沒有循環結構，Transformer 可以並行處理序列中的所有元素，大大加快了訓練速度。
*   **性能優越**：在自然語言處理領域取得了革命性成功，成為許多最先進模型的基礎（如 BERT, GPT 系列）。
*   **可解釋性**：注意力權重可以提供一定的可解釋性，顯示模型在做出決策時關注了序列中的哪些部分。

### 缺點

*   **計算成本高**：自注意力機制的計算複雜度與序列長度的平方成正比，對於非常長的序列，計算成本會很高。
*   **記憶消耗大**：對於長序列，儲存注意力權重矩陣需要大量的記憶。
*   **對位置信息敏感**：需要額外的位置編碼來引入序列順序信息。

---

## Transformer 的重要變體

*   **BERT (Bidirectional Encoder Representations from Transformers)**：一個基於 Transformer 編碼器的預訓練模型，透過雙向上下文學習語言表示，在多種 NLP 任務中取得了突破。
*   **GPT (Generative Pre-trained Transformer)**：一個基於 Transformer 解碼器的預訓練模型，擅長生成連貫的文本，是大型語言模型 (LLM) 的代表。

---

## 應用場景

*   **自然語言處理 (NLP)**：機器翻譯、文本摘要、問答系統、情感分析、文本生成。
*   **計算機視覺**：圖像分類、物體檢測（如 ViT, DETR）。
*   **語音識別**：語音轉文本。

---

Transformer 模型是深度學習領域的一個里程碑，它透過自注意力機制徹底改變了序列處理的方式。理解其自注意力、多頭注意力和位置編碼的原理是掌握它的關鍵。接下來，我們將探討生成對抗網路。請持續關注！
