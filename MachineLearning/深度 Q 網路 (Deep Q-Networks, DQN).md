# 深度 Q 網路 (Deep Q-Networks, DQN)

歡迎來到深度 Q 網路 (DQN) 的世界！DQN 是將**深度學習 (Deep Learning)** 的強大能力與**強化學習**中的 Q-learning 演算法相結合的里程碑式工作。它解決了傳統 Q-learning 在處理高維度狀態空間時面臨的「維度災難」問題，使得強化學習能夠成功應用於複雜的感知任務，例如直接從像素輸入玩 Atari 遊戲。

---

## 核心概念

*   **Q-learning 的局限性**：傳統 Q-learning 透過 Q 表來儲存每個狀態-動作對的 Q 值。當狀態空間非常大（例如圖像像素）時，Q 表會變得巨大且難以儲存和更新。
*   **深度神經網路作為 Q 函數近似器**：DQN 的核心思想是使用一個**深度神經網路 (Deep Neural Network, DNN)** 來近似 Q 函數 $Q(s, a)$。網路的輸入是狀態 $s$，輸出是每個可能動作 $a$ 的 Q 值。這樣，我們就不需要顯式地儲存 Q 表，而是讓網路學習如何從狀態直接預測 Q 值。
*   **經驗回放 (Experience Replay)**：為了解決深度學習訓練中的數據相關性問題，DQN 引入了經驗回放機制。代理人與環境互動產生的經驗（狀態 $s$、動作 $a$、獎勵 $r$、下一個狀態 $s'$）被儲存在一個**回放緩衝區 (Replay Buffer)** 中。在訓練時，我們從這個緩衝區中隨機抽取一批經驗來訓練網路。這有助於：
    *   **打破數據相關性**：隨機抽樣使得訓練數據更接近獨立同分佈 (i.i.d.)，這對深度學習模型的訓練至關重要。
    *   **提高數據利用率**：每個經驗可以被重複用於多次訓練。
*   **目標網路 (Target Network)**：為了解決訓練不穩定性問題，DQN 引入了兩個神經網路：
    *   **行為網路 (Policy Network)**：用於選擇動作和計算當前 Q 值。
    *   **目標網路 (Target Network)**：用於計算 Q-learning 更新中的目標 Q 值 $r + \gamma \max_{a'} Q(s', a')$。目標網路的參數會定期（而不是每一步）從行為網路複製過來，這使得目標值在一段時間內保持穩定，從而穩定訓練過程。

---

## DQN 的訓練過程

1.  **初始化**：
    *   初始化行為網路 $Q(s, a; \theta)$ 和目標網路 $Q(s, a; \theta_{target})$，其中 $\theta_{target} = \theta$。
    *   初始化回放緩衝區 $D$。
2.  **重複以下步驟，直到達到終止條件**：
    a.  **觀察初始狀態 $s$**。
    b.  **重複以下步驟，直到達到終止狀態**：
        i.   **選擇動作 $a$**：根據當前狀態 $s$ 和行為網路 $Q(s, a; \theta)$，使用 $\epsilon$-貪婪策略選擇一個動作 $a$。
        ii.  **執行動作 $a$**：在環境中執行動作 $a$，觀察環境返回的即時獎勵 $r$ 和下一個狀態 $s'$。
        iii. **儲存經驗**：將經驗 $(s, a, r, s')$ 儲存到回放緩衝區 $D$ 中。
        iv.  **更新狀態**：將 $s'$ 設為新的當前狀態 $s$。
        v.   **從回放緩衝區中採樣**：從 $D$ 中隨機抽取一批經驗 $(s_j, a_j, r_j, s'_j)$。
        vi.  **計算目標 Q 值**：對於每個採樣的經驗，計算目標 Q 值 $y_j = r_j + \gamma \max_{a'} Q(s'_j, a'; \theta_{target})$。
        vii. **更新行為網路**：使用梯度下降（例如 Adam 優化器）更新行為網路的參數 $\theta$，以最小化預測 Q 值 $Q(s_j, a_j; \theta)$ 與目標 Q 值 $y_j$ 之間的均方誤差 (MSE)。
        viii. **定期更新目標網路**：每隔 $C$ 步，將行為網路的參數 $\theta$ 複製到目標網路 $\theta_{target}$。

---

## 優點與缺點

### 優點

*   **處理高維狀態空間**：透過深度神經網路，DQN 能夠直接從原始高維輸入（如圖像像素）中學習，解決了 Q-learning 的維度災難問題。
*   **經驗回放**：打破了數據相關性，提高了數據利用率，穩定訓練過程。
*   **目標網路**：進一步穩定訓練過程，防止 Q 值估計的震盪。
*   **在許多複雜任務中表現出色**：例如在 Atari 遊戲中達到人類水平的表現。

### 缺點

*   **無法處理連續動作空間**：DQN 仍然需要離散的動作空間，因為它需要為每個動作輸出一個 Q 值。
*   **過度估計 Q 值**：DQN 傾向於過度估計 Q 值，這可能導致次優策略。後續的 Double DQN 解決了這個問題。
*   **訓練複雜**：涉及多個網路和緩衝區，調參相對複雜。
*   **收斂速度**：對於非常複雜的環境，訓練仍然可能很慢。

---

## 應用場景

*   **遊戲 AI**：Atari 遊戲、Go 等。
*   **機器人控制**：從視覺輸入學習控制策略。
*   **推薦系統**：基於用戶行為和環境狀態進行動態推薦。

---

DQN 是強化學習領域的一個重大突破，它成功地將深度學習引入了強化學習，為解決複雜的感知和控制問題開闢了道路。理解其經驗回放和目標網路的機制是掌握它的關鍵。接下來，我們將轉向基於策略的強化學習演算法。請持續關注！
