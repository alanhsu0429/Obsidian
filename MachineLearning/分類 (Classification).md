# 分類 (Classification)

歡迎來到分類的世界！在監督式學習中，當我們的目標是預測一個**離散型類別**時，我們所處理的問題就屬於分類問題。

---

## 核心概念

*   **離散型目標變數 (Discrete Target Variable)**：分類模型的輸出是一個類別標籤。例如，判斷郵件是否為垃圾郵件（是/否）、識別圖片中的物體（貓/狗/鳥）、診斷疾病（陽性/陰性）等。
*   **特徵 (Features)**：用於預測目標類別的輸入數據。例如，在判斷郵件是否為垃圾郵件時，郵件的發件人、主題、內容關鍵字等都是特徵。
*   **決策邊界 (Decision Boundary)**：分類模型會學習如何在特徵空間中劃分出不同的區域，每個區域對應一個類別。這些劃分線或面就是決策邊界。
*   **機率預測 (Probability Prediction)**：許多分類模型不僅能給出最終的類別預測，還能給出屬於每個類別的機率，這對於某些應用場景非常有用。

---

## 分類的目標

分類的目標是找到一個最佳的函數 $f(X)$，使得 $Y = f(X)$，其中 $Y$ 是目標類別，$X$ 是輸入特徵。這個函數能夠將輸入數據映射到正確的類別標籤。

---

## 常見的分類演算法

以下是一些常見的分類演算法，我們將在後續的節點中詳細介紹：

1.  **[[邏輯迴歸 (Logistic Regression)]]**：儘管名稱中帶有「迴歸」，但它是一個廣泛用於二元分類的線性模型，透過 Sigmoid 函數將線性輸出轉換為機率。
2.  **[[支持向量機 (Support Vector Machine, SVM)]]**：尋找一個最佳的超平面 (Hyperplane) 來最大化不同類別之間的邊界 (Margin)，從而實現分類。
3.  **[[決策樹分類 (Decision Tree Classification)]]**：透過樹狀結構進行決策，根據特徵值將數據逐步劃分到不同的類別。
4.  **[[隨機森林分類 (Random Forest Classification)]]**：由多個決策樹組成的集成學習模型，透過投票機制來決定最終的分類結果。
5.  **[[K-近鄰演算法 (K-Nearest Neighbors, KNN)]]**：一種基於實例的學習方法，透過查找與新數據點最接近的 K 個訓練數據點的類別來進行預測。
6.  **[[樸素貝葉斯 (Naive Bayes)]]**：基於貝葉斯定理和特徵條件獨立性假設的分類器，常用於文本分類。
7.  **[[類神經網路 (Artificial Neural Networks, ANN)]]**：受生物神經網路啟發的模型，透過多層次的非線性變換來學習複雜的模式，是深度學習的基礎。

---

## 評估指標

評估分類模型性能的常見指標比迴歸模型更為豐富，因為涉及到類別的正確判斷：

*   **準確率 (Accuracy)**：正確預測的樣本數佔總樣本數的比例。
*   **精確率 (Precision)**：在所有被預測為正類的樣本中，真正是正類的比例。
*   **召回率 (Recall)**：在所有真正是正類的樣本中，被模型正確預測為正類的比例。
*   **F1-分數 (F1-Score)**：精確率和召回率的調和平均數，綜合考慮了兩者。
*   **混淆矩陣 (Confusion Matrix)**：一個表格，用於可視化分類演算法的性能，顯示了真陽性 (TP)、真陰性 (TN)、假陽性 (FP) 和假陰性 (FN) 的數量。
*   **ROC 曲線 (Receiver Operating Characteristic Curve) 與 AUC (Area Under the Curve)**：用於評估二元分類器在不同閾值下的性能。

---

理解分類是許多實際應用中判斷類別的關鍵。接下來，我們將深入探討各種分類演算法。請持續關注！
