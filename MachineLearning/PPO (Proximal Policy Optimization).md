# PPO (Proximal Policy Optimization)

歡迎來到 PPO (Proximal Policy Optimization) 的世界！PPO 是一種**同策略 (On-Policy)** 的 Actor-Critic 強化學習演算法，它在穩定性和樣本效率之間取得了很好的平衡。PPO 因其**實現相對簡單、性能優越且穩定**而廣受歡迎，是目前強化學習領域中最常用的演算法之一。

---

## 核心概念

*   **同策略 (On-Policy)**：PPO 學習的是當前策略的價值函數和策略本身。這意味著用於訓練的數據必須是從當前策略中採樣得到的。
*   **Actor-Critic 結構**：PPO 遵循 Actor-Critic 的基本結構，包含一個**演員 (Actor)** 網路（學習策略）和一個**評論家 (Critic)** 網路（學習價值函數）。
*   **重要性採樣 (Importance Sampling)**：PPO 的一個關鍵創新是利用重要性採樣來允許策略在多個訓練步驟中重複使用相同的數據，而不會導致策略更新過大，從而提高樣本效率。它透過一個**機率比率 (Probability Ratio)** 來衡量新舊策略之間的差異。
*   **裁剪目標函數 (Clipped Objective Function)**：PPO 引入了一個裁剪機制，限制了策略更新的幅度。這可以防止策略在單次更新中偏離舊策略太遠，從而提高訓練的穩定性。
    *   **機率比率 $r_t(\theta)$**：新策略 $\pi_{\theta}(a_t|s_t)$ 與舊策略 $\pi_{\theta_{old}}(a_t|s_t)$ 之間的機率比率：
        $r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$
    *   **裁剪函數**：PPO 的目標函數包含一個裁剪項，它將機率比率 $r_t(\theta)$ 限制在 $[1 - \epsilon, 1 + \epsilon]$ 的範圍內，其中 $\epsilon$ 是一個超參數（通常為 0.1 或 0.2）。
        $L^{CLIP}(\theta) = \hat{E}_t [\min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t)]$
        其中 $\hat{A}_t$ 是優勢估計。
*   **廣義優勢估計 (Generalized Advantage Estimation, GAE)**：PPO 通常使用 GAE 來估計優勢函數，這是一種結合了蒙特卡洛方法和時間差分方法的優勢估計器，可以在偏差和方差之間取得更好的平衡。
*   **多個 Epoch 的小批量更新**：PPO 會在收集到一批數據後，對這些數據進行多次小批量更新，這進一步提高了樣本效率。

---

## PPO 的工作原理

1.  **初始化**：
    *   初始化演員網路（策略函數 $\pi(a|s; \theta)$ 的參數 $\theta$）。
    *   初始化評論家網路（價值函數 $V(s; w)$ 的參數 $w$）。
2.  **收集數據**：
    a.  代理人根據當前策略 $\pi_{\theta_{old}}$ 與環境互動，收集一批經驗（狀態、動作、獎勵、下一個狀態）。
    b.  計算每個時間步的優勢估計 $\hat{A}_t$（通常使用 GAE）。
3.  **優化**：
    a.  對於收集到的這批數據，進行多個 Epoch 的小批量更新。
    b.  在每個小批量更新中：
        i.   **更新評論家**：評論家網路的參數 $w$ 透過最小化預測價值與實際回報之間的均方誤差來更新。
        ii.  **更新演員**：演員網路的參數 $\theta$ 透過最大化裁剪目標函數 $L^{CLIP}(\theta)$ 來更新。同時，為了鼓勵探索，會添加一個熵項。
4.  **更新舊策略**：在完成對這批數據的所有更新後，將當前策略的參數複製到舊策略 $\theta_{old}$。
5.  **重複**：重複步驟 2-4，直到策略收斂或達到最大迭代次數。

---

## 優點與缺點

### 優點

*   **穩定性高**：裁剪目標函數限制了策略更新的幅度，使得訓練過程非常穩定，不易崩潰。
*   **樣本效率高**：透過重要性採樣和多個 Epoch 的小批量更新，PPO 能夠有效地利用收集到的數據。
*   **實現相對簡單**：相對於其他一些複雜的強化學習演算法，PPO 的實現相對簡單。
*   **性能優越**：在許多基準測試和實際應用中，PPO 都取得了非常好的性能。

### 缺點

*   **同策略**：仍然是同策略演算法，需要從當前策略中採樣數據，這在某些環境中可能不如異策略演算法的樣本效率高。
*   **超參數調整**：裁剪參數 $\epsilon$、GAE 參數、學習率等超參數的選擇仍然需要仔細調整。

---

## 應用場景

*   **機器人控制**：學習複雜的運動和操作策略。
*   **遊戲 AI**：在複雜的遊戲環境中學習策略，例如 OpenAI Five (Dota 2) 和 AlphaStar (StarCraft II) 都使用了 PPO 的變體。
*   **自動駕駛**：學習駕駛決策。

---

PPO 是強化學習領域的一個重要突破，它在穩定性、樣本效率和性能之間取得了卓越的平衡，是當前解決複雜控制問題的首選演算法之一。理解其裁剪目標函數和重要性採樣的原理是掌握它的關鍵。至此，我們已經涵蓋了強化學習的主要演算法。接下來，我將審視所有已建立的節點，並確保所有內容都已完善。請持續關注！
