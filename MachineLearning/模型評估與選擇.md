# 模型評估與選擇 (Model Evaluation and Selection)

歡迎來到模型評估與選擇的世界！在機器學習中，建立模型只是第一步。更關鍵的是，我們需要一套科學的方法來**評估模型的好壞**，並在眾多模型中**選擇最適合特定任務的模型**。這確保了模型不僅在訓練數據上表現良好，而且在未見過的新數據上也能保持穩定的性能。

---

## 核心概念

*   **訓練集 (Training Set)**：用於訓練模型，讓模型從中學習數據的模式和關係。
*   **驗證集 (Validation Set)**：用於調整模型的超參數 (Hyperparameters) 和進行初步的模型選擇。它在訓練過程中提供了一個獨立的性能評估，以避免在測試集上過擬合。
*   **測試集 (Test Set)**：用於最終評估模型的泛化能力。它必須是模型在訓練和驗證過程中從未見過的數據，以提供對模型在實際應用中性能的無偏估計。
*   **過擬合 (Overfitting)**：模型在訓練數據上表現極好，但在新數據上表現很差。這通常是因為模型學習了訓練數據中的噪聲和特有模式，而不是普遍規律。
*   **欠擬合 (Underfitting)**：模型在訓練數據和新數據上都表現很差。這通常是因為模型過於簡單，無法捕捉數據中的基本模式。
*   **偏差 (Bias)**：模型預測的平均值與真實值之間的差異。高偏差通常導致欠擬合。
*   **方差 (Variance)**：模型預測在不同訓練集上的變動程度。高方差通常導致過擬合。
*   **偏差-方差權衡 (Bias-Variance Trade-off)**：在模型複雜度、偏差和方差之間取得平衡是機器學習的關鍵挑戰。通常，降低偏差會增加方差，反之亦然。

---

## 評估指標

評估指標的選擇取決於任務類型（迴歸、分類等）。

### 迴歸模型評估指標

*   **[[均方誤差 (Mean Squared Error, MSE)]]**：預測值與真實值之差的平方的平均值。
*   **[[均方根誤差 (Root Mean Squared Error, RMSE)]]**：MSE 的平方根，與目標變數的單位相同，更具可解釋性。
*   **[[平均絕對誤差 (Mean Absolute Error, MAE)]]**：預測值與真實值之差的絕對值的平均值。
*   **[[R-平方 (R-squared, $R^2$)]]**：衡量模型解釋目標變數變異的比例，值越接近 1 表示模型擬合得越好。

### 分類模型評估指標

*   **[[準確率 (Accuracy)]]**：正確預測的樣本數佔總樣本數的比例。
*   **[[精確率 (Precision)]]**：在所有被預測為正類的樣本中，真正是正類的比例。
*   **[[召回率 (Recall)]]**：在所有真正是正類的樣本中，被模型正確預測為正類的比例。
*   **[[F1-分數 (F1-Score)]]**：精確率和召回率的調和平均數，綜合考慮了兩者。
*   **[[混淆矩陣 (Confusion Matrix)]]**：一個表格，用於可視化分類演算法的性能，顯示了真陽性 (TP)、真陰性 (TN)、假陽性 (FP) 和假陰性 (FN) 的數量。
*   **[[ROC 曲線 (Receiver Operating Characteristic Curve) 與 AUC (Area Under the Curve)]]**：用於評估二元分類器在不同閾值下的性能。

---

## 模型選擇技術

*   **[[交叉驗證 (Cross-Validation)]]**：一種評估模型泛化能力的技術，透過將數據集劃分為多個子集，輪流作為訓練集和驗證集，以獲得更穩健的性能估計。
    *   **K 折交叉驗證 (K-Fold Cross-Validation)**：最常用的交叉驗證方法。
*   **網格搜索 (Grid Search)**：一種超參數優化技術，透過遍歷所有可能的超參數組合來找到最佳模型。
*   **隨機搜索 (Random Search)**：在超參數空間中隨機採樣，通常比網格搜索更高效。
*   **貝葉斯優化 (Bayesian Optimization)**：一種更智能的超參數優化方法，它利用過去的評估結果來指導下一次的超參數選擇。

---

## 應用場景

*   **任何機器學習項目**：模型評估與選擇是機器學習工作流程中不可或缺的一部分。

---

理解模型評估與選擇的原則和技術是建立可靠、高性能機器學習模型的關鍵。接下來，我們將深入探討過擬合與欠擬合。請持續關注！
