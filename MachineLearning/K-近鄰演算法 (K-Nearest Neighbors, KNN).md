# K-近鄰演算法 (K-Nearest Neighbors, KNN)

歡迎來到 K-近鄰演算法 (KNN) 的世界！KNN 是一種簡單、直觀且廣泛應用的**非參數 (Non-parametric)** 機器學習演算法，可用於**分類**和**迴歸**問題。它的核心思想是「物以類聚」，即一個樣本的類別或值由其最近的 $K$ 個鄰居的類別或值決定。

---

## 核心概念

*   **惰性學習 (Lazy Learning)**：KNN 是一種惰性學習演算法，這意味著它在訓練階段不做任何模型學習，只是簡單地儲存所有的訓練數據。所有的計算都在預測階段進行。
*   **距離度量 (Distance Metric)**：KNN 的核心是計算樣本之間的距離。常用的距離度量包括：
    *   **歐幾里得距離 (Euclidean Distance)**：最常見的距離度量，適用於連續型特徵。
    *   **曼哈頓距離 (Manhattan Distance)**：也稱為城市街區距離，適用於連續型特徵。
    *   **漢明距離 (Hamming Distance)**：適用於類別型特徵。
*   **K 值 (Number of Neighbors)**：這是 KNN 演算法中最重要的超參數。它決定了在預測時考慮多少個最近的鄰居。K 值的大小對模型的性能有顯著影響：
    *   **K 值過小**：模型容易受到噪聲和異常值的影響，導致過擬合。
    *   **K 值過大**：模型會變得過於平滑，可能無法捕捉數據的局部結構，導致欠擬合。
*   **投票機制 (Voting Mechanism)**：
    *   **分類問題**：對於一個新的數據點，KNN 會找出其最近的 $K$ 個鄰居，然後將這些鄰居中出現次數最多的類別作為該數據點的預測類別（多數投票）。
    *   **迴歸問題**：對於一個新的數據點，KNN 會找出其最近的 $K$ 個鄰居，然後將這些鄰居的目標變數值的平均值作為該數據點的預測值。

---

## KNN 的工作原理

對於一個新的、未知的數據點 $X_{new}$：

1.  **計算距離**：計算 $X_{new}$ 與訓練數據集中所有數據點之間的距離。
2.  **選擇鄰居**：根據計算出的距離，選擇與 $X_{new}$ 最接近的 $K$ 個數據點。
3.  **進行預測**：
    *   **分類**：統計這 $K$ 個鄰居中各類別的數量，將數量最多的類別作為 $X_{new}$ 的預測類別。
    *   **迴歸**：計算這 $K$ 個鄰居的目標變數值的平均值，作為 $X_{new}$ 的預測值。

---

## 優點與缺點

### 優點

*   **簡單易懂**：演算法概念直觀，易於理解和實現。
*   **無需訓練**：沒有顯式的訓練階段，所有計算都在預測時進行。
*   **非參數**：對數據的分佈沒有假設，適用於各種數據類型。
*   **處理多類別問題**：可以自然地擴展到多類別分類問題。

### 缺點

*   **計算成本高**：在預測階段，需要計算新數據點與所有訓練數據點的距離，對於大型數據集，計算量非常大，效率低下。
*   **對特徵尺度敏感**：由於距離度量是基於特徵值的，因此特徵的尺度會嚴重影響距離計算。通常需要對特徵進行標準化或歸一化。
*   **對高維數據敏感 (維度災難)**：在高維空間中，所有數據點之間的距離趨於相等，使得「最近鄰」的概念變得模糊，導致性能下降。
*   **需要選擇 K 值**：K 值的選擇對模型性能影響很大，需要透過交叉驗證等方法進行優化。

---

## 應用場景

*   推薦系統
*   圖像識別、手寫數字識別
*   文本分類
*   異常檢測

---

K-近鄰演算法是一種基礎且實用的機器學習方法。儘管它存在一些缺點，但在許多場景下仍然是一個強有力的基準模型。理解其工作原理和超參數 K 的選擇是掌握它的關鍵。接下來，我們將探討樸素貝葉斯分類器。請持續關注！
