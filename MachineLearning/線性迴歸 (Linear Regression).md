# 線性迴歸 (Linear Regression)

歡迎來到線性迴歸的世界！線性迴歸是機器學習中最基本、最廣泛使用的迴歸演算法之一。它的核心思想是假設輸入特徵與目標變數之間存在**線性關係**。

---

## 核心概念

*   **線性關係 (Linear Relationship)**：模型假設目標變數 $Y$ 可以透過輸入特徵 $X$ 的線性組合來預測。這意味著當 $X$ 變化時，$Y$ 也會以一個固定的比例變化。
*   **模型方程 (Model Equation)**：
    *   **簡單線性迴歸 (Simple Linear Regression)**（一個特徵）：
        $Y = b_0 + b_1X_1 + \epsilon$
        其中：
        *   $Y$ 是目標變數（因變數）。
        *   $X_1$ 是輸入特徵（自變數）。
        *   $b_0$ 是截距 (Intercept)，表示當 $X_1=0$ 時 $Y$ 的預測值。
        *   $b_1$ 是斜率 (Slope) 或係數 (Coefficient)，表示 $X_1$ 每增加一個單位，$Y$ 平均變化多少。
        *   $\epsilon$ 是誤差項 (Error Term)，代表模型無法解釋的隨機變動。
    *   **多元線性迴歸 (Multiple Linear Regression)**（多個特徵）：
        $Y = b_0 + b_1X_1 + b_2X_2 + \dots + b_nX_n + \epsilon$
        其中：
        *   $X_1, X_2, \dots, X_n$ 是多個輸入特徵。
        *   $b_1, b_2, \dots, b_n$ 是對應的係數。
*   **最小平方法 (Ordinary Least Squares, OLS)**：線性迴歸最常用的參數估計方法。它的目標是找到一組係數 $b_0, b_1, \dots, b_n$，使得所有數據點的**殘差平方和 (Sum of Squared Residuals, SSR)** 最小化。殘差是實際值 $Y$ 與預測值 $\hat{Y}$ 之間的差異。

---

## 線性迴歸的假設

為了使線性迴歸模型的結果有效且可靠，數據通常需要滿足以下幾個假設：

1.  **線性關係 (Linearity)**：特徵與目標變數之間存在線性關係。
2.  **獨立性 (Independence)**：觀測值之間相互獨立，沒有相關性。
3.  **常態性 (Normality)**：殘差項 $\epsilon$ 服從常態分佈。
4.  **同方差性 (Homoscedasticity)**：殘差的方差在所有特徵值上都是常數，即殘差的分散程度不隨預測值變化。
5.  **無多重共線性 (No Multicollinearity)**：輸入特徵之間不應存在高度相關性。高度相關的特徵會導致係數估計不穩定。

---

## 優點與缺點

### 優點

*   **簡單易懂**：模型解釋性強，係數的意義直觀。
*   **計算效率高**：訓練速度快，尤其適用於大型數據集。
*   **廣泛應用**：在統計學和機器學習領域都有著悠久的歷史和廣泛的應用。

### 缺點

*   **假設嚴格**：對數據的假設較多，如果數據不滿足這些假設，模型性能可能不佳。
*   **無法捕捉非線性關係**：如果特徵與目標變數之間存在複雜的非線性關係，線性迴歸可能表現不佳。
*   **對異常值敏感**：最小平方法會對異常值給予較大的權重，導致模型容易受到異常值的影響。

---

## 應用場景

*   房價預測
*   銷售額預測
*   股票價格趨勢分析
*   醫學研究中藥物劑量與療效的關係

---

理解線性迴歸是理解更複雜迴歸模型的基礎。接下來，我們將探討如何處理非線性關係以及如何改進線性迴歸的穩定性。請持續關注！
