# Actor-Critic (演員-評論家)

歡迎來到 Actor-Critic 的世界！Actor-Critic 是一種**強化學習**演算法，它巧妙地結合了**基於策略 (Policy-based)** 和**基於價值 (Value-based)** 方法的優點。你可以將它想像成一個團隊：**演員 (Actor)** 負責學習和執行策略（即選擇動作），而**評論家 (Critic)** 則負責評估演員的表現（即估計價值函數），並提供反饋來指導演員改進策略。

---

## 核心概念

*   **演員 (Actor)**：
    *   **策略函數 (Policy Function, $\pi(a|s; \theta)$)**：演員是一個參數化的策略函數（通常是神經網路），它接收狀態 $s$ 作為輸入，輸出在該狀態下採取每個動作 $a$ 的機率分佈。演員的目標是學習一個最佳策略，以最大化預期累積獎勵。
    *   **負責探索**：演員透過其策略來探索環境，生成動作。
*   **評論家 (Critic)**：
    *   **價值函數 (Value Function, $V(s; w)$ 或 $Q(s, a; w)$)**：評論家是一個參數化的價值函數（通常是另一個神經網路），它接收狀態 $s$ 作為輸入，輸出該狀態的價值 $V(s)$，或者接收狀態 $s$ 和動作 $a$ 作為輸入，輸出該狀態-動作對的 Q 值 $Q(s, a)$。評論家的目標是準確地估計價值函數。
    *   **負責評估**：評論家評估演員所採取動作的好壞，並提供一個「基線」(Baseline) 或「優勢函數」(Advantage Function) 來減少策略梯度的方差。
*   **優勢函數 (Advantage Function, $A(s, a)$)**：這是 Actor-Critic 的關鍵概念之一。它衡量了在特定狀態 $s$ 下採取特定動作 $a$ 比平均情況好多少。優勢函數通常定義為：
    $A(s, a) = Q(s, a) - V(s)$
    或者更常用於更新策略：
    $A(s, a) = r + \gamma V(s') - V(s)$ （這被稱為**時間差誤差 (Temporal Difference Error, TD Error)**）
    優勢函數的作用是減少策略梯度的方差，使訓練更穩定。
*   **策略梯度 (Policy Gradient)**：演員的策略更新仍然基於策略梯度定理，但現在梯度估計中引入了評論家的價值估計，以減少方差。
    $ abla_{\theta} J(\theta) = E_{\pi_{\theta}} [\nabla_{\theta} \log \pi(a_t|s_t; \theta) A(s_t, a_t)]$

---

## Actor-Critic 的工作原理

1.  **初始化**：
    *   初始化演員網路（策略函數 $\pi(a|s; \theta)$ 的參數 $\theta$）。
    *   初始化評論家網路（價值函數 $V(s; w)$ 或 $Q(s, a; w)$ 的參數 $w$）。
2.  **互動與學習**：
    a.  **演員選擇動作**：在當前狀態 $s_t$ 下，演員根據其策略 $\pi(a|s_t; \theta)$ 選擇一個動作 $a_t$。
    b.  **環境反饋**：執行動作 $a_t$，環境返回即時獎勵 $r_{t+1}$ 和下一個狀態 $s_{t+1}$。
    c.  **評論家更新**：評論家使用 $r_{t+1} + \gamma V(s_{t+1}; w)$ 作為目標，更新其價值函數 $V(s; w)$ 的參數 $w$，以最小化預測價值與實際回報之間的誤差（例如，使用 TD 誤差作為損失函數）。
    d.  **演員更新**：演員使用評論家提供的優勢函數 $A(s_t, a_t)$（通常是 TD 誤差）來更新其策略 $\pi(a|s; \theta)$ 的參數 $\theta$，以最大化預期累積獎勵。
3.  **重複**：重複步驟 2，直到策略收斂或達到最大迭代次數。

---

## 優點與缺點

### 優點

*   **結合兩者優勢**：結合了基於策略方法的直接學習策略和基於價值方法利用價值函數來減少方差的優點。
*   **減少方差**：評論家提供的價值估計（優勢函數）作為基線，顯著減少了策略梯度的方差，使得訓練更穩定、收斂更快。
*   **處理連續動作空間**：與純策略梯度方法一樣，可以自然地處理連續動作空間。
*   **同策略或異策略**：根據具體的實現，可以是同策略或異策略。

### 缺點

*   **偏差與方差的權衡**：雖然評論家減少了方差，但如果評論家的價值估計不準確，可能會引入偏差。
*   **兩個網路的訓練**：需要同時訓練兩個網路（演員和評論家），這增加了模型的複雜度和訓練難度。
*   **超參數調整**：有更多的超參數需要調整，例如兩個網路的學習率、折扣因子等。

---

## Actor-Critic 的變體

Actor-Critic 是一個廣泛的框架，有許多重要的變體，它們在結構和更新方式上有所不同：

*   **[[A2C (Advantage Actor-Critic)]]**：同步的、單線程的 Actor-Critic 實現。
*   **[[A3C (Asynchronous Advantage Actor-Critic)]]**：A2C 的異步版本，透過多個並行運行的代理人來加速訓練和提高探索效率。
*   **[[DDPG (Deep Deterministic Policy Gradient)]]**：一種用於連續動作空間的異策略 Actor-Critic 演算法，結合了 DQN 的思想。
*   **[[PPO (Proximal Policy Optimization)]]**：一種高效且穩定的同策略 Actor-Critic 演算法，透過限制策略更新的幅度來提高訓練穩定性。

---

## 應用場景

*   **機器人控制**：學習複雜的運動和操作策略。
*   **自動駕駛**：學習駕駛決策。
*   **遊戲 AI**：在複雜的遊戲環境中學習策略。

---

Actor-Critic 框架是現代強化學習領域的核心，它為解決複雜的控制問題提供了強大的工具。理解演員和評論家之間的協同作用是掌握它的關鍵。接下來，我們將深入探討其重要的變體。請持續關注！
