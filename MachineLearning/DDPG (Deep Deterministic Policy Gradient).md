# DDPG (Deep Deterministic Policy Gradient)

歡迎來到 DDPG (Deep Deterministic Policy Gradient) 的世界！DDPG 是一種**異策略 (Off-Policy)** 的 Actor-Critic 演算法，專為**連續動作空間 (Continuous Action Spaces)** 設計。它結合了 DQN 的思想（使用深度網路近似 Q 函數、經驗回放、目標網路）和策略梯度方法（直接學習策略），使得強化學習能夠處理更複雜的連續控制任務。

---

## 核心概念

*   **異策略 (Off-Policy)**：DDPG 學習的是最佳策略，但它在學習過程中可以採用探索性策略來收集數據。這使得它能夠利用經驗回放緩衝區中的數據，提高樣本效率。
*   **連續動作空間 (Continuous Action Spaces)**：這是 DDPG 的主要優勢之一。傳統的 Q-learning 和 DQN 只能處理離散動作空間，因為它們需要為每個動作輸出一個 Q 值。DDPG 透過學習一個**確定性策略 (Deterministic Policy)** 來解決這個問題。
*   **確定性策略 (Deterministic Policy)**：DDPG 的演員網路輸出的是一個確定的動作值，而不是動作的機率分佈。這使得它在連續動作空間中選擇動作變得簡單。
*   **Actor-Critic 結構**：DDPG 包含四個深度神經網路：
    1.  **行為演員網路 (Actor Network)**：學習確定性策略 $\mu(s; \theta^{\mu})$，將狀態 $s$ 映射到動作 $a$。
    2.  **行為評論家網路 (Critic Network)**：學習動作價值函數 $Q(s, a; \theta^Q)$，評估在狀態 $s$ 下執行動作 $a$ 的 Q 值。
    3.  **目標演員網路 (Target Actor Network)**：行為演員網路的延遲副本，用於計算目標 Q 值中的下一個動作。
    4.  **目標評論家網路 (Target Critic Network)**：行為評論家網路的延遲副本，用於計算目標 Q 值。
*   **經驗回放 (Experience Replay)**：與 DQN 類似，DDPG 也使用經驗回放緩衝區來儲存經驗，並從中隨機採樣進行訓練，以打破數據相關性並提高樣本效率。
*   **目標網路 (Target Networks)**：DDPG 使用目標演員網路和目標評論家網路來穩定訓練。目標網路的參數會以軟更新 (Soft Update) 的方式（例如，每次更新一小部分）從行為網路複製過來，而不是硬性複製。
    $\theta^{Q'} \leftarrow \tau \theta^Q + (1 - \tau) \theta^{Q'}$
    $\theta^{\mu'} \leftarrow \tau \theta^{\mu} + (1 - \tau) \theta^{\mu'}$
    其中 $\tau$ 是一個很小的更新率。
*   **探索 (Exploration)**：由於演員網路輸出的是確定性動作，DDPG 需要額外的機制來鼓勵探索。通常會在演員輸出的動作中添加**噪聲 (Noise)**（例如，Ornstein-Uhlenbeck 過程噪聲），以確保代理人能夠探索環境。

---

## DDPG 的訓練過程

1.  **初始化**：
    *   初始化行為演員網路 $\mu(s; \theta^{\mu})$ 和行為評論家網路 $Q(s, a; \theta^Q)$ 的參數。
    *   初始化目標演員網路 $\mu'(s; \theta^{\mu'})$ 和目標評論家網路 $Q'(s, a; \theta^{Q'})$，並將其參數複製自行為網路。
    *   初始化經驗回放緩衝區 $R$。
2.  **重複以下步驟，直到達到終止條件**：
    a.  **觀察初始狀態 $s_t$**。
    b.  **重複以下步驟，直到達到終止狀態**：
        i.   **選擇動作 $a_t$**：根據當前狀態 $s_t$ 和行為演員網路 $\mu(s_t; \theta^{\mu})$ 輸出一個確定性動作，並添加探索噪聲。
        ii.  **執行動作 $a_t$**：在環境中執行動作 $a_t$，觀察環境返回的即時獎勵 $r_t$ 和下一個狀態 $s_{t+1}$。
        iii. **儲存經驗**：將經驗 $(s_t, a_t, r_t, s_{t+1})$ 儲存到回放緩衝區 $R$ 中。
        iv.  **從回放緩衝區中採樣**：從 $R$ 中隨機抽取一批經驗 $(s_j, a_j, r_j, s_{j+1})$。
        v.   **計算目標 Q 值**：
            $y_j = r_j + \gamma Q'(s_{j+1}, \mu'(s_{j+1}; \theta^{\mu'}); \theta^{Q'})$
        vi.  **更新行為評論家網路**：透過最小化損失 $L = \frac{1}{N} \sum_j (y_j - Q(s_j, a_j; \theta^Q))^2$ 來更新 $\theta^Q$。
        vii. **更新行為演員網路**：透過策略梯度 $
    abla_{\theta^{\mu}} J \approx \frac{1}{N} \sum_j \nabla_a Q(s, a; \theta^Q)|_{s=s_j, a=\mu(s_j)} \nabla_{\theta^{\mu}} \mu(s; \theta^{\mu})|_{s=s_j}$ 來更新 $\theta^{\mu}$。
        viii. **軟更新目標網路**：更新目標演員網路和目標評論家網路的參數。

---

## 優點與缺點

### 優點

*   **處理連續動作空間**：這是 DDPG 的主要優勢，使其能夠應用於許多實際的連續控制任務。
*   **異策略**：可以利用經驗回放，提高樣本效率。
*   **穩定性高**：結合了 DQN 的穩定化技術（經驗回放、目標網路）。

### 缺點

*   **對超參數敏感**：學習率、折扣因子、噪聲參數、軟更新率等超參數的選擇對性能影響很大。
*   **探索效率**：確定性策略的探索依賴於添加噪聲，這可能不是最有效的探索方式。
*   **過度估計 Q 值**：與 DQN 類似，DDPG 也可能存在 Q 值過度估計的問題。

---

## 應用場景

*   **機器人控制**：學習複雜的連續動作控制，例如機械臂操作、行走機器人。
*   **自動駕駛**：學習車輛的加速、剎車、轉向等連續控制。
*   **金融交易**：學習連續的買賣策略。

---

DDPG 是解決連續控制問題的強大工具，它為強化學習在現實世界中的應用開闢了新的道路。理解其確定性策略、經驗回放和目標網路的協同作用是掌握它的關鍵。接下來，我們將探討 PPO。請持續關注！
