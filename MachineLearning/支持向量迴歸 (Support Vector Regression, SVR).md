# 支持向量迴歸 (Support Vector Regression, SVR)

歡迎來到支持向量迴歸 (SVR) 的世界！SVR 是**支持向量機 (SVM)** 在迴歸問題上的應用。與傳統迴歸模型試圖最小化預測值與真實值之間的誤差不同，SVR 的目標是找到一個函數，使得所有數據點都盡可能地靠近這個函數，並且在一定容忍範圍內的誤差不被懲罰。

---

## 核心概念

*   **$\epsilon$-不敏感損失函數 ($\epsilon$-Insensitive Loss Function)**：這是 SVR 的核心思想。SVR 不關心預測值與真實值之間小於 $\epsilon$ (epsilon) 的誤差。只有當誤差超過這個閾值 $\epsilon$ 時，才會產生懲罰。這使得 SVR 對於訓練數據中的噪聲和異常值具有一定的魯棒性。
*   **超平面 (Hyperplane)**：在 SVR 中，我們尋找一個「最佳」的超平面，它不僅能擬合數據，還能確保大多數數據點都落在一個由超平面定義的「邊界」之內。
*   **支持向量 (Support Vectors)**：那些位於邊界上或邊界之外的數據點被稱為支持向量。它們是決定超平面位置的關鍵點，因為它們對模型的構建影響最大。
*   **核函數 (Kernel Function)**：SVR 能夠處理非線性關係，透過將數據映射到更高維的特徵空間，在這個高維空間中尋找線性關係。常用的核函數包括線性核 (Linear Kernel)、多項式核 (Polynomial Kernel)、徑向基函數核 (Radial Basis Function Kernel, RBF) 等。

---

## SVR 的目標

SVR 的目標是找到一個函數 $f(x)$，使得：

1.  對於訓練數據中的所有 $x_i$，預測值 $f(x_i)$ 與真實值 $y_i$ 之間的絕對誤差 $|y_i - f(x_i)|$ 小於或等於 $\epsilon$ 的數據點，不產生損失。
2.  對於誤差大於 $\epsilon$ 的數據點，損失是 $|y_i - f(x_i)| - \epsilon$。
3.  同時，模型希望最大化超平面與邊界之間的距離（即最小化模型的複雜度），並最小化超出 $\epsilon$ 範圍的誤差。

這可以透過最小化以下目標函數來實現：

$\min_{w, b, \xi, \xi^*} \frac{1}{2}||w||^2 + C \sum_{i=1}^{m} (\xi_i + \xi_i^*)$

約束條件：

$y_i - w \cdot \phi(x_i) - b \le \epsilon + \xi_i$
$w \cdot \phi(x_i) + b - y_i \le \epsilon + \xi_i^*$
$\xi_i, \xi_i^* \ge 0$

其中：
*   $w$ 是超平面的法向量。
*   $b$ 是截距。
*   $\phi(x_i)$ 是將 $x_i$ 映射到高維空間的核函數。
*   $\xi_i, \xi_i^*$ 是鬆弛變量 (Slack Variables)，用於衡量超出 $\epsilon$ 範圍的誤差。
*   $C$ 是懲罰參數，控制著模型對超出 $\epsilon$ 範圍誤差的懲罰程度。$C$ 越大，模型越傾向於不允許誤差超出 $\epsilon$ 範圍。

---

## SVR 的優點與缺點

### 優點

*   **處理非線性關係**：透過核函數，SVR 能夠有效地處理特徵與目標變數之間的非線性關係。
*   **對異常值魯棒**：$\epsilon$-不敏感損失函數使得 SVR 對於訓練數據中的噪聲和異常值具有較好的魯棒性。
*   **泛化能力強**：SVR 的目標是最小化結構風險 (Structural Risk)，而不是經驗風險 (Empirical Risk)，這有助於提高模型的泛化能力。

### 缺點

*   **計算成本高**：對於大型數據集，SVR 的訓練時間可能較長。
*   **參數選擇困難**：SVR 有多個超參數需要調整，例如 $C$、$\epsilon$ 和核函數的參數（如 RBF 核的 $\gamma$），這需要經驗和交叉驗證。
*   **解釋性較差**：由於核函數將數據映射到高維空間，模型的解釋性不如線性迴歸直觀。

---

## 應用場景

*   時間序列預測 (如股票價格、電力負荷)
*   生物信息學中的蛋白質結構預測
*   金融領域的風險評估
*   環境科學中的污染預測

---

支持向量迴歸是一種強大且靈活的迴歸模型，尤其擅長處理複雜的非線性數據。理解其核心思想和參數調整對於成功應用 SVR 至關重要。接下來，我們將探討另一種強大的集成學習方法——梯度提升迴歸。請持續關注！
