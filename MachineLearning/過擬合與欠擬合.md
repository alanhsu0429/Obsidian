# 過擬合與欠擬合 (Overfitting and Underfitting)

歡迎來到過擬合與欠擬合的世界！在機器學習模型的訓練過程中，我們經常會遇到這兩個問題。理解它們的成因、影響以及如何解決，是建立穩健、高性能模型的關鍵。

---

## 核心概念

*   **過擬合 (Overfitting)**：
    *   **定義**：模型在**訓練數據**上表現得非常好，但在**未見過的新數據**（例如驗證集或測試集）上表現得很差。這意味著模型過度學習了訓練數據中的噪聲和特有模式，而不是數據背後的普遍規律。
    *   **表現**：訓練誤差很低，但驗證誤差或測試誤差很高。
    *   **原因**：
        *   **模型複雜度過高**：模型參數過多，或者模型結構過於複雜（例如，決策樹深度過深，神經網路層數過多）。
        *   **訓練數據量不足**：模型沒有足夠的數據來學習普遍規律，反而記住了個別樣本的特徵。
        *   **訓練時間過長**：在某些情況下，模型訓練時間過長也可能導致過擬合。
    *   **比喻**：學生死記硬背了所有考題的答案，但沒有真正理解知識點，導致在遇到新考題時無法作答。

*   **欠擬合 (Underfitting)**：
    *   **定義**：模型在**訓練數據**和**未見過的新數據**上都表現得很差。這意味著模型過於簡單，無法捕捉數據中的基本模式和趨勢。
    *   **表現**：訓練誤差很高，驗證誤差或測試誤差也很高。
    *   **原因**：
        *   **模型複雜度過低**：模型參數過少，或者模型結構過於簡單（例如，使用線性模型去擬合非線性數據）。
        *   **特徵不足**：輸入特徵無法提供足夠的信息來預測目標變數。
        *   **訓練時間過短**：模型還沒有充分學習數據中的模式。
    *   **比喻**：學生對知識點一知半解，導致在面對任何考題時都無法作答。

---

## 偏差-方差權衡 (Bias-Variance Trade-off)

過擬合和欠擬合與模型的**偏差 (Bias)** 和**方差 (Variance)** 密切相關：

*   **偏差 (Bias)**：模型預測的平均值與真實值之間的差異。高偏差通常導致欠擬合，因為模型過於簡單，無法準確捕捉數據的真實關係。
*   **方差 (Variance)**：模型預測在不同訓練集上的變動程度。高方差通常導致過擬合，因為模型對訓練數據的微小變化過於敏感，導致在不同數據集上表現不穩定。

在模型複雜度、偏差和方差之間取得平衡是機器學習的關鍵挑戰。通常，降低偏差會增加方差，反之亦然。我們的目標是找到一個「甜點」，使得模型的總誤差（偏差平方 + 方差 + 不可約誤差）最小。

---

## 解決過擬合與欠擬合的策略

### 解決過擬合 (Reducing Overfitting)

1.  **增加訓練數據量**：更多的數據可以幫助模型學習到更普遍的規律，而不是噪聲。
2.  **降低模型複雜度**：
    *   減少模型參數數量。
    *   對於決策樹，進行**剪枝 (Pruning)** 或限制樹的深度。
    *   對於神經網路，減少層數或每層神經元數量。
3.  **特徵選擇 (Feature Selection)**：移除不相關或冗餘的特徵。
4.  **正則化 (Regularization)**：在損失函數中添加懲罰項，限制模型參數的大小，例如 [[嶺迴歸 (Ridge Regression)]] (L2 正則化) 和 [[Lasso迴歸 (Lasso Regression)]] (L1 正則化)。
5.  **集成學習 (Ensemble Learning)**：
    *   **袋裝法 (Bagging)**：如 [[隨機森林 (Random Forest)]]，透過訓練多個模型並取平均來減少方差。
    *   **提升法 (Boosting)**：如 [[梯度提升 (Gradient Boosting)]]，透過迭代地訓練弱學習器來減少偏差。
6.  **早停法 (Early Stopping)**：在訓練過程中，監控模型在驗證集上的性能。當驗證誤差開始上升時，停止訓練，以防止過擬合。
7.  **Dropout (神經網路)**：在神經網路訓練時，隨機丟棄一部分神經元，迫使網路學習更魯棒的特徵。

### 解決欠擬合 (Reducing Underfitting)

1.  **增加模型複雜度**：
    *   增加模型參數數量。
    *   對於決策樹，增加樹的深度。
    *   對於神經網路，增加層數或每層神經元數量。
    *   使用更複雜的模型（例如，從線性模型轉向非線性模型，如[[多項式迴歸 (Polynomial Regression)]]）。
2.  **增加特徵**：
    *   進行**特徵工程 (Feature Engineering)**，從現有特徵中創造新的、更有信息量的特徵。
    *   收集更多相關的特徵。
3.  **減少正則化強度**：如果模型使用了正則化，可以嘗試減小正則化參數。
4.  **延長訓練時間**：確保模型有足夠的時間來學習數據中的模式。

---

## 應用場景

*   **所有機器學習模型的開發和調優**：理解過擬合和欠擬合是成功建立機器學習模型的基礎。

---

理解過擬合與欠擬合是機器學習實踐中最重要的概念之一。透過適當的策略來平衡模型的複雜度，我們可以建立出在實際應用中表現良好的模型。接下來，我們將探討特徵工程。請持續關注！
