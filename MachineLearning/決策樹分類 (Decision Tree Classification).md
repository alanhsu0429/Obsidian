# 決策樹分類 (Decision Tree Classification)

歡迎來到決策樹分類的世界！決策樹不僅可以用於迴歸問題，同樣也是一種強大的**分類**模型。它透過一系列簡單的決策規則來預測離散型目標變數，將數據劃分到不同的類別中。你可以將它想像成一個流程圖，根據數據的特徵逐步做出分類判斷。

---

## 核心概念

*   **樹狀結構 (Tree Structure)**：與決策樹迴歸類似，決策樹分類也由節點 (Nodes) 和邊 (Edges) 組成。
    *   **根節點 (Root Node)**：樹的頂部，代表整個數據集。
    *   **內部節點 (Internal Node)**：代表一個特徵的測試條件（例如「年齡是否大於 30 歲？」）。
    *   **分支 (Branch)**：測試條件的結果，將數據導向不同的子節點。
    *   **葉節點 (Leaf Node)**：樹的末端，代表最終的類別預測（例如「是」或「否」）。
*   **遞歸二元分割 (Recursive Binary Splitting)**：決策樹的建立過程是透過不斷地將數據集分割成兩個子集來進行的。每次分割的目標是使得分割後的子集內部數據的**純度 (Purity)** 更高（即子集中的樣本盡可能屬於同一個類別）。
*   **分割標準 (Splitting Criterion)**：在分類樹中，常用的分割標準是：
    *   **基尼不純度 (Gini Impurity)**：衡量一個集合中隨機選取的兩個樣本標籤不一致的機率。基尼不純度越低，集合的純度越高。
    *   **信息增益 (Information Gain)**：基於熵 (Entropy) 的概念。熵衡量一個集合的混亂程度。信息增益衡量分割前後熵的減少量。信息增益越大，分割效果越好。
    *   **信息增益比 (Gain Ratio)**：信息增益的改進版本，用於避免偏向於選擇取值多的特徵。
*   **預測 (Prediction)**：對於一個新的數據點，它會從根節點開始，根據特徵值沿著樹的分支向下遍歷，直到到達一個葉節點。該葉節點中佔比最多的類別就是模型的預測結果。

---

## 決策樹分類的建立過程

1.  從根節點開始，考慮所有特徵和所有可能的分割點。
2.  選擇一個特徵和分割點，使得分割後兩個子節點的純度最高（例如，基尼不純度最小或信息增益最大）。
3.  將數據集根據選定的分割點分成兩個子集。
4.  對每個子集遞歸地重複步驟 1-3，直到滿足停止條件（例如，節點中的樣本數小於某個閾值，或者樹的深度達到最大值，或者節點已經足夠純淨）。
5.  每個葉節點的預測類別是該節點中所有訓練樣本中佔比最多的類別。

---

## 優點與缺點

### 優點

*   **易於理解和解釋**：樹狀結構直觀，可以很容易地可視化和解釋決策過程。
*   **無需特徵縮放**：對數據的尺度不敏感，不需要進行特徵標準化或歸一化。
*   **能處理非線性關係**：透過多層次的決策，可以捕捉特徵與目標變數之間的複雜非線性關係。
*   **能處理類別型特徵**：可以直接處理類別型特徵，無需進行獨熱編碼 (One-Hot Encoding)。

### 缺點

*   **容易過擬合 (Overfitting)**：單一決策樹很容易過度擬合訓練數據，尤其是在樹的深度較大時。這會導致模型在未見過的數據上表現不佳。
*   **不穩定性 (Instability)**：數據的微小變化（例如，增加或刪除幾個樣本）可能導致樹的結構發生巨大變化。
*   **局部最優 (Local Optimality)**：決策樹的建立是貪婪的，每次分割都選擇當前最優的，但不保證全局最優。

---

## 防止過擬合的策略

*   **剪枝 (Pruning)**：在樹建立完成後，移除一些不重要的分支，以簡化樹的結構。
*   **限制樹的深度 (Max Depth)**：設定樹的最大深度，防止樹生長過深。
*   **限制葉節點的最小樣本數 (Min Samples Leaf)**：設定葉節點必須包含的最小樣本數，避免生成過於細小的葉節點。

---

## 應用場景

*   客戶流失預測
*   信用風險評估
*   醫學診斷
*   垃圾郵件過濾

---

決策樹分類提供了一種直觀且強大的分類能力，但其過擬合的傾向需要我們特別注意。接下來，我們將探討如何透過集成學習來克服單一決策樹的缺點，例如隨機森林。請持續關注！
