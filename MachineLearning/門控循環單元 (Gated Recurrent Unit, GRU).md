# 門控循環單元 (Gated Recurrent Unit, GRU)

歡迎來到門控循環單元 (GRU) 的世界！GRU 是**循環神經網路 (RNN)** 的另一種重要變體，由 Cho 等人於 2014 年提出。它與[[長短期記憶網路 (Long Short-Term Memory, LSTM)]]一樣，旨在解決傳統 RNN 的**梯度消失 (Vanishing Gradient)** 問題，並有效地捕捉**長距離依賴關係 (Long-Term Dependencies)**。GRU 的設計比 LSTM 更為簡潔，參數更少，但通常在許多任務上能達到與 LSTM 相似的性能。

---

## 核心概念

*   **簡化的門控機制 (Simplified Gating Mechanism)**：GRU 只有兩個門，而不是 LSTM 的三個門，這使得它的計算效率更高。
    1.  **更新門 (Update Gate, $z_t$)**：決定有多少過去的信息（來自 $h_{t-1}$）需要傳遞到當前時間步，以及有多少新的信息（來自 $x_t$）需要被採納。它結合了 LSTM 的遺忘門和輸入門的功能。
        $z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$
    2.  **重置門 (Reset Gate, $r_t$)**：決定有多少過去的隱藏狀態 $h_{t-1}$ 需要被「遺忘」或「重置」。如果重置門的值接近 0，則表示忽略過去的隱藏狀態。
        $r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$
*   **候選隱藏狀態 (Candidate Hidden State, $\tilde{h}_t$)**：根據當前輸入 $x_t$ 和經過重置門處理的過去隱藏狀態 $h_{t-1}$ 來計算。它代表了新的、潛在的隱藏狀態。
    $\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$
    其中 $\odot$ 表示元素級乘法。
*   **更新隱藏狀態**：最終的隱藏狀態 $h_t$ 是過去隱藏狀態 $h_{t-1}$ 和候選隱藏狀態 $\tilde{h}_t$ 的加權平均，權重由更新門 $z_t$ 控制。
    $h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$

---

## GRU 的優點與缺點

### 優點

*   **解決梯度消失問題**：與 LSTM 一樣，GRU 也能有效地解決梯度消失問題，捕捉長距離依賴關係。
*   **計算效率高**：由於門控機制更簡潔，參數更少，GRU 的訓練速度通常比 LSTM 快。
*   **性能與 LSTM 相當**：在許多任務上，GRU 能夠達到與 LSTM 相似的性能，甚至在某些情況下表現更好。
*   **記憶能力強**：能夠選擇性地記憶和遺忘信息。

### 缺點

*   **模型解釋性差**：與 LSTM 類似，GRU 的決策過程也難以解釋。
*   **仍然存在梯度爆炸問題**：雖然解決了梯度消失，但梯度爆炸問題仍然可能存在，通常需要使用梯度裁剪 (Gradient Clipping) 來解決。

---

## GRU 與 LSTM 的比較

| 特性         | LSTM                                  | GRU                                   |
| :----------- | :------------------------------------ | :------------------------------------ |
| **門的數量** | 3 個門 (遺忘門、輸入門、輸出門)       | 2 個門 (更新門、重置門)               |
| **細胞狀態** | 獨立的細胞狀態 $C_t$                  | 沒有獨立的細胞狀態，隱藏狀態 $h_t$ 兼具記憶功能 |
| **複雜度**   | 較高，參數較多                        | 較低，參數較少                        |
| **計算效率** | 較低                                  | 較高                                  |
| **性能**     | 在許多任務上表現優異                  | 在許多任務上與 LSTM 相當，甚至更好    |

---

## 應用場景

*   **自然語言處理 (NLP)**：機器翻譯、文本生成、情感分析。
*   **語音識別**：將語音轉換為文本。
*   **時間序列預測**：股票價格預測、天氣預報。

---

門控循環單元是循環神經網路家族中一個非常實用且高效的成員。理解其簡潔的門控機制和與 LSTM 的異同是掌握它的關鍵。接下來，我們將探討雙向循環神經網路。請持續關注！
