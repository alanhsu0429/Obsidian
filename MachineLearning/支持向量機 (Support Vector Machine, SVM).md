# 支持向量機 (Support Vector Machine, SVM)

歡迎來到支持向量機 (SVM) 的世界！支持向量機是一種強大且靈活的**監督式學習**模型，廣泛應用於**分類**和迴歸問題。其核心思想是找到一個最佳的**超平面 (Hyperplane)**，將不同類別的數據點最大程度地分開。

---

## 核心概念

*   **超平面 (Hyperplane)**：在 $N$ 維空間中，一個超平面是一個 $N-1$ 維的子空間。在二維空間中，它是一條直線；在三維空間中，它是一個平面。SVM 的目標是找到一個超平面，能夠將不同類別的數據點分開。
*   **最大邊界 (Maximum Margin)**：SVM 的獨特之處在於它不僅僅是找到一個能分開數據的超平面，而是找到一個**最大化邊界**的超平面。邊界是指超平面與最近的訓練數據點之間的距離。最大化邊界有助於提高模型的泛化能力。
*   **支持向量 (Support Vectors)**：那些距離超平面最近的訓練數據點，它們位於邊界上或邊界之內。這些點對超平面的位置和方向起著決定性的作用。如果移除這些支持向量，超平面可能會改變。
*   **核技巧 (Kernel Trick)**：SVM 能夠處理**非線性可分**的數據。透過核函數，SVM 可以將原始數據映射到一個更高維的特徵空間，在這個高維空間中，數據點可能變得線性可分。常用的核函數包括：
    *   **線性核 (Linear Kernel)**：適用於數據本身就是線性可分的情況。
    *   **多項式核 (Polynomial Kernel)**：適用於數據具有多項式關係的情況。
    *   **徑向基函數核 (Radial Basis Function Kernel, RBF 或 Gaussian Kernel)**：最常用的核函數之一，能夠處理非常複雜的非線性關係。
    *   **Sigmoid 核 (Sigmoid Kernel)**：類似於類神經網路中的激活函數。
*   **軟邊界 (Soft Margin)**：在實際應用中，數據往往不是完全線性可分的，或者存在一些噪聲和異常值。軟邊界允許一些數據點落在邊界之內，甚至跨越超平面，但會對這些「違規」的點施加懲罰。這透過引入**鬆弛變量 (Slack Variables)** 和**懲罰參數 $C$** 來實現。
    *   **懲罰參數 $C$**：控制著模型對錯誤分類的懲罰程度。$C$ 值越大，模型對錯誤分類的懲罰越重，邊界越窄，可能導致過擬合；$C$ 值越小，模型對錯誤分類的容忍度越高，邊界越寬，可能導致欠擬合。

---

## SVM 的目標

SVM 的目標是找到一組參數 $(w, b)$，使得超平面 $w \cdot x + b = 0$ 能夠最大化邊界，同時最小化分類錯誤。這可以透過最小化以下目標函數來實現：

$\min_{w, b, \xi} \frac{1}{2}||w||^2 + C \sum_{i=1}^{m} \xi_i$

約束條件：

$y_i (w \cdot \phi(x_i) + b) \ge 1 - \xi_i$
$\xi_i \ge 0$

其中：
*   $w$ 是超平面的法向量。
*   $b$ 是截距。
*   $\phi(x_i)$ 是將 $x_i$ 映射到高維空間的核函數。
*   $\xi_i$ 是鬆弛變量，衡量數據點違反邊界的程度。
*   $C$ 是懲罰參數。

---

## 優點與缺點

### 優點

*   **處理高維數據**：在處理高維數據時表現出色，即使特徵數量大於樣本數量也能有效工作。
*   **泛化能力強**：透過最大化邊界，SVM 傾向於具有良好的泛化能力，不易過擬合。
*   **處理非線性關係**：透過核技巧，能夠有效地處理非線性可分的數據。
*   **對異常值魯棒**：軟邊界機制使其對訓練數據中的噪聲和異常值具有一定的魯棒性。

### 缺點

*   **計算成本高**：對於大型數據集，SVM 的訓練時間可能較長，尤其是在使用複雜核函數時。
*   **參數選擇困難**：核函數的選擇和超參數 $C$、核參數（如 RBF 核的 $\gamma$）的調整對模型性能影響很大，需要經驗和交叉驗證。
*   **解釋性較差**：由於核函數將數據映射到高維空間，模型的解釋性不如邏輯迴歸直觀。

---

## 應用場景

*   文本分類、情感分析
*   圖像識別、手寫數字識別
*   生物信息學中的蛋白質分類
*   醫學診斷

---

支持向量機是一種非常強大且理論基礎堅實的分類模型。理解其最大邊界和核技巧是掌握其精髓的關鍵。接下來，我們將探討決策樹在分類問題上的應用。請持續關注！
