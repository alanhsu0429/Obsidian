# A2C (Advantage Actor-Critic)

歡迎來到 A2C (Advantage Actor-Critic) 的世界！A2C 是 Actor-Critic 框架的一個同步、單線程實現。它結合了**策略梯度 (Policy Gradient)** 和**價值函數 (Value Function)** 的優點，透過使用**優勢函數 (Advantage Function)** 來減少策略梯度的方差，從而實現更穩定和高效的訓練。

---

## 核心概念

*   **Actor-Critic 框架**：A2C 遵循 Actor-Critic 的基本結構，包含一個**演員 (Actor)** 網路（學習策略）和一個**評論家 (Critic)** 網路（學習價值函數）。
*   **優勢函數 (Advantage Function)**：A2C 的名稱來源於其對優勢函數的明確使用。優勢函數 $A(s, a)$ 衡量了在特定狀態 $s$ 下採取特定動作 $a$ 比平均情況好多少。在 A2C 中，優勢函數通常透過**時間差誤差 (Temporal Difference Error, TD Error)** 來估計：
    $A(s_t, a_t) = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$
    其中 $V(s_t)$ 是評論家網路對狀態 $s_t$ 價值的估計。
*   **策略更新**：演員網路的策略參數 $\theta$ 透過優勢函數來更新，目標是最大化預期累積獎勵：
    $ 
abla_{\theta} J(\theta) = E [\nabla_{\theta} \log \pi(a|s; \theta) A(s, a)]$
*   **價值函數更新**：評論家網路的價值函數參數 $w$ 透過最小化預測價值 $V(s)$ 與實際回報（或 TD 目標）之間的均方誤差來更新。
*   **熵正則化 (Entropy Regularization)**：為了鼓勵探索，A2C 通常會在演員的損失函數中添加一個**熵項 (Entropy Term)**。熵衡量了策略的隨機性。增加熵可以防止策略過早收斂到次優解，並有助於探索。

---

## A2C 的工作原理

1.  **初始化**：
    *   初始化演員網路（策略函數 $\pi(a|s; \theta)$ 的參數 $\theta$）。
    *   初始化評論家網路（價值函數 $V(s; w)$ 的參數 $w$）。
2.  **互動與學習**：
    a.  **生成軌跡**：代理人根據當前策略 $\pi(a|s; \theta)$ 與環境互動，生成一個短期的軌跡（例如，幾個時間步或直到終止狀態）。
    b.  **計算優勢函數**：對於軌跡中的每個時間步 $t$，計算 TD 誤差作為優勢函數 $A(s_t, a_t) = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$。
    c.  **更新評論家**：評論家網路的參數 $w$ 透過最小化 $A(s_t, a_t)^2$ 來更新，即最小化預測價值與實際回報之間的誤差。
    d.  **更新演員**：演員網路的參數 $\theta$ 透過最大化 $A(s_t, a_t) \log \pi(a_t|s_t; \theta)$ 來更新。同時，為了鼓勵探索，會添加一個熵項。
3.  **重複**：重複步驟 2，直到策略收斂或達到最大迭代次數。

---

## 優點與缺點

### 優點

*   **穩定性高**：透過優勢函數減少了策略梯度的方差，使得訓練過程比純策略梯度方法更穩定。
*   **效率較高**：相對於蒙特卡洛策略梯度，A2C 採用了 TD 誤差，可以進行更頻繁的更新，提高了樣本效率。
*   **處理連續動作空間**：可以自然地處理連續動作空間。
*   **熵正則化促進探索**：有助於防止策略陷入局部最優。

### 缺點

*   **對超參數敏感**：學習率、折扣因子、熵係數等超參數的選擇對性能影響很大。
*   **兩個網路的訓練**：需要同時訓練兩個網路，增加了模型的複雜度。
*   **仍然可能存在方差**：儘管優勢函數減少了方差，但仍然可能存在。

---

## A2C 與 A3C 的關係

A2C 可以被視為 A3C (Asynchronous Advantage Actor-Critic) 的同步版本。A3C 透過多個並行運行的代理人來加速訓練和提高探索效率，而 A2C 則是在單個線程中進行訓練，但通常會使用批處理 (Batch Processing) 來提高效率。

---

## 應用場景

*   **遊戲 AI**：在複雜的遊戲環境中學習策略。
*   **機器人控制**：學習複雜的運動和操作策略。
*   **資源管理**：優化資源分配和調度。

---

A2C 是 Actor-Critic 框架的一個堅實基礎，理解其優勢函數和同步訓練的原理是掌握它的關鍵。接下來，我們將探討其異步版本 A3C。請持續關注！
