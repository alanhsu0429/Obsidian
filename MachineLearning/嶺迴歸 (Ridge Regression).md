# 嶺迴歸 (Ridge Regression)

歡迎來到嶺迴歸的世界！嶺迴歸是**線性迴歸**的一種改進，它透過引入**L2 正則化 (L2 Regularization)** 來解決標準線性迴歸中可能出現的**過擬合 (Overfitting)** 和**多重共線性 (Multicollinearity)** 問題。

---

## 核心概念

*   **正則化 (Regularization)**：一種用於防止模型過擬合的技術。它透過在模型的損失函數中添加一個懲罰項，來限制模型參數（係數）的大小，從而使模型更簡單、更穩定。
*   **L2 正則化 (L2 Regularization)**：嶺迴歸使用的正則化類型。它在損失函數中添加了所有係數平方和的懲罰項。這個懲罰項會促使模型將係數縮小，但不會將它們完全縮小到零。
*   **損失函數 (Loss Function)**：
    標準線性迴歸的損失函數（最小平方法）：
    $J(\beta) = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{m} (y_i - (\beta_0 + \sum_{j=1}^{n} \beta_j x_{ij}))^2$

    嶺迴歸的損失函數：
    $J_{ridge}(\beta) = \sum_{i=1}^{m} (y_i - (\beta_0 + \sum_{j=1}^{n} \beta_j x_{ij}))^2 + \lambda \sum_{j=1}^{n} \beta_j^2$
    其中：
    *   $\beta$ 代表模型的係數向量。
    *   $m$ 是樣本數量，$n$ 是特徵數量。
    *   $y_i$ 是真實值，$\hat{y}_i$ 是預測值。
    *   $\lambda$ (lambda) 是**正則化參數 (Regularization Parameter)**，它控制著懲罰項的強度。$\lambda$ 越大，係數被縮小的程度越大，模型越簡單；$\lambda$ 越小，嶺迴歸越接近標準線性迴歸。

---

## 嶺迴歸的優點與缺點

### 優點

*   **處理多重共線性**：當特徵之間存在高度相關性時，嶺迴歸可以穩定係數的估計，使其對數據的微小變化不那麼敏感。
*   **防止過擬合**：透過縮小係數，嶺迴歸可以降低模型的複雜度，從而減少過擬合的風險，提高模型的泛化能力。
*   **保留所有特徵**：嶺迴歸會縮小所有特徵的係數，但不會將任何係數完全縮小到零，這意味著所有特徵都會保留在模型中。

### 缺點

*   **不具備特徵選擇能力**：由於不會將係數縮小到零，嶺迴歸無法直接進行特徵選擇，即無法自動識別並剔除不重要的特徵。
*   **解釋性略有下降**：雖然係數仍然存在，但由於它們被縮小，其直接解釋性可能不如標準線性迴歸。

---

## 正則化參數 $\lambda$ 的選擇

$\lambda$ 是一個超參數 (Hyperparameter)，需要透過交叉驗證 (Cross-validation) 等方法來選擇最佳值。通常會嘗試一系列不同的 $\lambda$ 值，並選擇在驗證集上表現最好的模型。

---

## 應用場景

*   當數據集中存在大量相關特徵時。
*   當模型出現過擬合的跡象時。
*   在基因組學、金融建模等領域，特徵數量遠大於樣本數量的情況。

---

嶺迴歸是處理線性模型中過擬合和多重共線性問題的有效方法。接下來，我們將探討另一種正則化技術——Lasso 迴歸，它在特徵選擇方面有著獨特的優勢。請持續關注！
