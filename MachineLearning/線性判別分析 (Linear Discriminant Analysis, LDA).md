# 線性判別分析 (Linear Discriminant Analysis, LDA)

歡迎來到線性判別分析 (LDA) 的世界！LDA 是一種**有監督的降維**方法，與 PCA 不同，它在降維時會考慮數據的類別信息。LDA 的目標是找到一個投影方向，使得**不同類別之間的距離最大化，同時同一類別內部的距離最小化**，從而實現最佳的分類效果。

---

## 核心概念

*   **有監督降維 (Supervised Dimensionality Reduction)**：LDA 需要數據的類別標籤來進行降維。這是它與 PCA 的主要區別，PCA 是一種無監督方法。
*   **類間散度 (Between-class Scatter)**：衡量不同類別之間的分散程度。LDA 旨在最大化類間散度。
*   **類內散度 (Within-class Scatter)**：衡量同一類別內部數據點的分散程度。LDA 旨在最小化類內散度。
*   **投影 (Projection)**：LDA 將高維數據投影到一個低維空間中，使得在這個新空間中，不同類別的數據點能夠更好地分離。
*   **最大化類間散度與類內散度之比**：LDA 的數學目標是找到一個投影向量 $w$，使得投影後類間散度與類內散度之比最大化。

---

## LDA 的工作原理

1.  **計算每個類別的均值向量 (Mean Vector)**：對於每個類別，計算其所有數據點的均值向量。
2.  **計算類內散度矩陣 (Within-class Scatter Matrix, $S_W$)**：衡量每個類別內部數據點的分散程度，並將它們加總。
3.  **計算類間散度矩陣 (Between-class Scatter Matrix, $S_B$)**：衡量不同類別均值向量之間的分散程度。
4.  **計算最佳投影向量**：找到一個投影向量 $w$，使得 $w^T S_B w / w^T S_W w$ 最大化。這可以透過求解廣義特徵值問題來實現。
5.  **數據投影 (Projection)**：將原始數據投影到由選定的投影向量構成的新空間中，得到降維後的數據。

---

## LDA 與 PCA 的比較

| 特性         | PCA                               | LDA                                   |
| :----------- | :-------------------------------- | :------------------------------------ |
| **目的**     | 最大化數據的總方差                | 最大化類間散度與類內散度之比          |
| **類型**     | 無監督降維                        | 有監督降維                            |
| **信息利用** | 只利用數據的方差信息              | 利用數據的方差信息和類別信息          |
| **結果**     | 找到數據變異最大的方向            | 找到最能區分類別的方向                |
| **應用**     | 特徵提取、數據壓縮、可視化        | 特徵提取、分類預處理                  |

---

## 優點與缺點

### 優點

*   **考慮類別信息**：在降維時考慮了數據的類別信息，因此在分類任務中通常比 PCA 表現更好。
*   **提高分類性能**：透過最大化類別之間的分離度，有助於提高後續分類模型的性能。
*   **減少維度**：有效減少數據的維度，降低計算複雜度。

### 缺點

*   **線性方法**：LDA 是一種線性降維方法，無法捕捉數據中的非線性結構。
*   **對數據分佈有假設**：假設每個類別的數據服從高斯分佈，且具有相同的協方差矩陣。如果這些假設不成立，性能可能下降。
*   **對異常值敏感**：均值向量的計算對異常值敏感。
*   **最多只能降到 $C-1$ 維**：其中 $C$ 是類別的數量。如果類別數量很少，降維的程度有限。

---

## 應用場景

*   **人臉識別**：從人臉圖像中提取特徵，用於分類。
*   **醫學診斷**：根據病理數據區分不同疾病。
*   **語音識別**：語音特徵的降維。

---

線性判別分析是一種強大的有監督降維工具，尤其適用於需要區分不同類別的任務。理解其最大化類間散度與最小化類內散度之比的原理是掌握它的關鍵。接下來，我們將探討另一種非線性降維方法——t-SNE。請持續關注！
