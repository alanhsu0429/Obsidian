# 循環神經網路 (Recurrent Neural Networks, RNNs)

歡迎來到循環神經網路 (RNNs) 的世界！RNNs 是**深度學習**中一種特殊設計的[[類神經網路 (Artificial Neural Networks, ANN)]]架構，它專門用於處理**序列數據 (Sequential Data)**，例如文本、語音、時間序列等。與傳統神經網路不同，RNNs 具有「記憶」能力，能夠捕捉序列中的時間依賴關係。

---

## 核心概念

*   **循環連接 (Recurrent Connections)**：RNN 的核心特點是其隱藏層的輸出不僅傳遞給下一層，還會傳遞回自身，作為下一個時間步的輸入。這使得網路能夠在處理序列數據時，將過去的信息「記憶」下來，並影響當前的輸出。
*   **隱藏狀態 (Hidden State / Context State)**：在每個時間步 $t$，RNN 的隱藏層會產生一個隱藏狀態 $h_t$。這個 $h_t$ 包含了從序列開始到當前時間步 $t$ 的所有相關信息。它充當了網路的「記憶」。
*   **時間步 (Time Step)**：序列數據中的每個元素都對應一個時間步。RNN 在每個時間步處理一個輸入，並更新其隱藏狀態。
*   **權重共享 (Weight Sharing)**：與 CNN 類似，RNN 在所有時間步中共享相同的權重。這大大減少了模型的參數數量，並使得模型能夠處理任意長度的序列。
*   **梯度消失與梯度爆炸 (Vanishing and Exploding Gradients)**：這是傳統 RNN 在處理長序列時面臨的兩個主要挑戰：
    *   **梯度消失**：在反向傳播過程中，梯度會隨著時間步的增加而指數級減小，導致網路難以學習長距離依賴關係。
    *   **梯度爆炸**：梯度會隨著時間步的增加而指數級增大，導致網路參數更新過大，訓練不穩定。

---

## RNN 的典型架構

一個典型的 RNN 單元在時間步 $t$ 的計算如下：

$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$
$y_t = W_{hy}h_t + b_y$

其中：
*   $x_t$ 是時間步 $t$ 的輸入。
*   $h_t$ 是時間步 $t$ 的隱藏狀態，$h_{t-1}$ 是上一個時間步的隱藏狀態。
*   $y_t$ 是時間步 $t$ 的輸出。
*   $W_{hh}, W_{xh}, W_{hy}$ 是權重矩陣，$b_h, b_y$ 是偏置向量。這些參數在所有時間步中共享。
*   $f$ 是激活函數（例如 tanh 或 ReLU）。

---

## RNN 的變體

為了解決傳統 RNN 的梯度消失/爆炸問題，並更好地捕捉長距離依賴關係，出現了許多 RNN 的變體：

1.  **[[長短期記憶網路 (Long Short-Term Memory, LSTM)]]**：
    *   **特點**：引入了「門控機制」(Gating Mechanism)（輸入門、遺忘門、輸出門）和「細胞狀態」(Cell State)，能夠有效地控制信息的流動，從而解決了梯度消失問題，並更好地捕捉長距離依賴關係。
    *   **應用**：在自然語言處理、語音識別等領域取得了巨大成功。

2.  **[[門控循環單元 (Gated Recurrent Unit, GRU)]]**：
    *   **特點**：LSTM 的簡化版本，只有兩個門（更新門、重置門），參數更少，計算效率更高，但在許多任務上性能與 LSTM 相當。
    *   **應用**：與 LSTM 類似，常用於序列數據處理。

3.  **[[雙向循環神經網路 (Bidirectional RNN, BiRNN)]]**：
    *   **特點**：由兩個獨立的 RNN 組成，一個從前向後處理序列，另一個從後向前處理序列。這使得網路能夠同時利用過去和未來的信息來做出預測。
    *   **應用**：在需要上下文信息的任務中表現出色，如命名實體識別。

4.  **[[深度循環神經網路 (Deep RNN)]]**：
    *   **特點**：堆疊多個 RNN 層，使得網路能夠學習更複雜的層次化表示。

---

## 優點與缺點

### 優點

*   **處理序列數據**：專為處理序列數據而設計，能夠捕捉時間依賴關係。
*   **權重共享**：減少了參數數量，使得模型能夠處理任意長度的序列。
*   **在 NLP 和語音領域表現出色**：在機器翻譯、語音識別、文本生成等任務中取得了巨大成功。

### 缺點**

*   **梯度消失/爆炸**：傳統 RNN 在處理長序列時容易出現梯度消失或爆炸問題，難以學習長距離依賴關係（儘管 LSTM 和 GRU 解決了這個問題）。
*   **訓練速度慢**：由於序列的依賴性，RNN 的訓練過程是串行的，難以並行化。
*   **記憶容量有限**：即使是 LSTM 和 GRU，對於非常長的序列，其記憶容量仍然有限。

---

## 應用場景

*   **自然語言處理 (NLP)**：機器翻譯、文本生成、情感分析、命名實體識別。
*   **語音識別**：將語音轉換為文本。
*   **時間序列預測**：股票價格預測、天氣預報。
*   **視頻分析**：動作識別、行為預測。

---

循環神經網路是處理序列數據的強大工具，理解其循環連接和隱藏狀態的原理是掌握它的關鍵。接下來，我們將探討 Transformer 模型，它在序列處理方面帶來了革命性的變革。請持續關注！
