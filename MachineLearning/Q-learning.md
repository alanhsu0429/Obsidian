# Q-learning

歡迎來到 Q-learning 的世界！Q-learning 是一種**無模型 (Model-Free)**、**異策略 (Off-Policy)** 的**強化學習**演算法。它的核心思想是學習一個**動作價值函數 (Action-Value Function)**，通常稱為 **Q 函數 (Q-function)**，來評估在特定狀態下執行特定動作的長期回報。透過學習這個 Q 函數，代理人 (Agent) 就能夠在不知道環境動態的情況下，找到最佳的策略。

---

## 核心概念

*   **無模型 (Model-Free)**：Q-learning 不需要知道環境的轉移機率和獎勵函數。它透過與環境的互動來學習，這使得它在環境模型未知或難以建模的情況下非常有用。
*   **異策略 (Off-Policy)**：Q-learning 是一種異策略學習演算法。這意味著它學習的是**最佳策略 (Optimal Policy)** 的 Q 函數，但它在學習過程中可以採用**探索性策略 (Exploration Policy)**（例如 $\epsilon$-貪婪策略）來收集數據。這使得代理人可以在探索的同時學習最佳行為。
*   **Q 函數 (Q-function)**：$Q(s, a)$ 表示在狀態 $s$ 下執行動作 $a$ 所能獲得的**預期累積折扣獎勵 (Expected Cumulative Discounted Reward)**。代理人的目標是學習一個準確的 Q 函數，以便在任何狀態下都能選擇能最大化未來獎勵的動作。
*   **Bellman 方程 (Bellman Equation)**：Q-learning 的更新規則基於 Bellman 方程，它將當前狀態的價值與未來狀態的價值聯繫起來。
    $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$
    其中：
    *   $s$ 是當前狀態，$a$ 是在 $s$ 中執行的動作。
    *   $s'$ 是執行 $a$ 後到達的下一個狀態，$a'$ 是在 $s'$ 中可能執行的動作。
    *   $r$ 是執行 $a$ 後獲得的即時獎勵。
    *   $\alpha$ (alpha) 是**學習率 (Learning Rate)**，控制每次更新的步長。$\alpha$ 越大，模型對新信息的反應越快。
    *   $\gamma$ (gamma) 是**折扣因子 (Discount Factor)**，衡量未來獎勵的重要性。$\gamma$ 越接近 1，代理人越重視長期獎勵；$\gamma$ 越接近 0，代理人越重視即時獎勵。
    *   $\max_{a'} Q(s', a')$ 表示在下一個狀態 $s'$ 中，所有可能動作 $a'$ 中最大的 Q 值。這是 Q-learning 異策略的關鍵，它假設代理人總是選擇未來能帶來最大獎勵的動作，即使當前採取的動作不是基於這個假設。

---

## Q-learning 的工作原理

1.  **初始化 Q 表**：將所有狀態-動作對的 Q 值初始化為任意值（通常為零）。
2.  **觀察初始狀態 $s$**。
3.  **重複以下步驟，直到達到終止狀態或最大迭代次數**：
    a.  **選擇動作 $a$**：根據當前狀態 $s$ 和 Q 表，使用**探索性策略**（例如 $\epsilon$-貪婪策略）選擇一個動作 $a$。
        *   **$\epsilon$-貪婪策略 ($\epsilon$-Greedy Policy)**：以 $\epsilon$ 的機率隨機選擇一個動作（探索），以 $1-\epsilon$ 的機率選擇當前 Q 值最大的動作（利用）。隨著訓練的進行，$\epsilon$ 通常會逐漸減小，從而減少探索，增加利用。
    b.  **執行動作 $a$**：在環境中執行動作 $a$，觀察環境返回的即時獎勵 $r$ 和下一個狀態 $s'$。
    c.  **更新 Q 值**：使用 Bellman 方程更新 $Q(s, a)$ 的值。
    d.  **更新狀態**：將 $s'$ 設為新的當前狀態 $s$。

---

## 優點與缺點

### 優點

*   **無模型**：不需要知道環境的動態，可以直接從經驗中學習。
*   **異策略**：可以在探索的同時學習最佳策略，這使得它能夠在不遵循最佳策略的情況下找到最佳策略。
*   **收斂性**：在滿足一定條件（例如，所有狀態-動作對都被無限次訪問）下，Q-learning 可以收斂到最佳 Q 函數。

### 缺點

*   **Q 表維度災難**：對於狀態空間或動作空間非常大的問題，儲存和更新 Q 表會變得非常困難，甚至不可能。這就是所謂的「維度災難」。
*   **收斂速度慢**：對於複雜的環境，Q-learning 的收斂速度可能很慢。
*   **無法處理連續狀態/動作空間**：Q-learning 傳統上是為離散狀態和動作空間設計的。

---

## 應用場景

*   **遊戲 AI**：如 Atari 遊戲、簡單棋類遊戲。
*   **機器人控制**：在簡單的環境中學習導航和任務執行。
*   **資源管理**：優化資源分配。

---

Q-learning 是強化學習的基石之一，理解其 Q 函數和 Bellman 更新規則是掌握強化學習的關鍵。接下來，我們將探討另一種基於價值的演算法——SARSA。請持續關注！
