# REINFORCE (Monte Carlo Policy Gradient)

歡迎來到 REINFORCE 的世界！REINFORCE 是一種**基於策略 (Policy-based)** 的**強化學習**演算法，它直接學習一個**策略函數 (Policy Function)**，而不是像 Q-learning 那樣學習價值函數。策略函數直接將狀態映射到動作的機率分佈，或者直接映射到動作本身。REINFORCE 屬於**蒙特卡洛策略梯度 (Monte Carlo Policy Gradient)** 方法，它透過完整的軌跡 (Trajectory) 來估計梯度並更新策略。

---

## 核心概念

*   **策略函數 (Policy Function, $\pi(a|s; \theta)$)**：策略函數是一個參數化的函數（通常是神經網路），它接收狀態 $s$ 作為輸入，輸出在該狀態下採取每個動作 $a$ 的機率。我們的目標是調整策略的參數 $\theta$，以最大化預期累積獎勵。
*   **蒙特卡洛方法 (Monte Carlo Method)**：REINFORCE 是一種蒙特卡洛方法，這意味著它需要等待一個完整的**軌跡 (Trajectory)**（從起始狀態到終止狀態的一系列狀態-動作-獎勵序列）結束後，才能計算並更新策略。
*   **策略梯度 (Policy Gradient)**：策略梯度方法的核心思想是直接計算策略函數參數 $\theta$ 的梯度，然後沿著梯度方向更新 $\theta$，以最大化預期累積獎勵。
    $abla_{\theta} J(\theta) = E_{\pi_{\theta}} [\nabla_{\theta} \log \pi(a_t|s_t; \theta) G_t]$
    其中：
    *   $J(\theta)$ 是策略的性能指標（例如預期累積獎勵）。
    *   $E_{\pi_{\theta}}$ 表示在策略 $\pi_{\theta}$ 下的期望。
    *   $\log \pi(a_t|s_t; \theta)$ 是在狀態 $s_t$ 採取動作 $a_t$ 的機率的對數。
    *   $G_t$ 是從時間步 $t$ 開始的**累積折扣獎勵 (Return)**。它代表了從 $t$ 時刻開始，遵循策略 $\pi_{\theta}$ 所能獲得的總獎勵。
*   **梯度上升 (Gradient Ascent)**：由於我們的目標是最大化獎勵，所以我們使用梯度上升來更新策略參數，而不是梯度下降。
    $\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)$
    其中 $\alpha$ 是學習率。

---

## REINFORCE 的工作原理

1.  **初始化策略網路**：初始化策略函數的參數 $\theta$（例如，一個神經網路的權重）。
2.  **生成軌跡**：根據當前策略 $\pi_{\theta}$，讓代理人與環境互動，生成一個完整的軌跡：$s_0, a_0, r_1, s_1, a_1, r_2, \dots, s_T, a_T, r_{T+1}$。
3.  **計算累積獎勵**：對於軌跡中的每個時間步 $t$，計算從該時間步開始的累積折扣獎勵 $G_t$。
    $G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots + \gamma^{T-t} r_{T+1}$
4.  **計算策略梯度**：對於軌跡中的每個時間步 $t$，計算策略梯度項 $\nabla_{\theta} \log \pi(a_t|s_t; \theta) G_t$。
5.  **更新策略參數**：將所有時間步的策略梯度項加總，並使用梯度上升更新策略參數 $\theta$。
    $\theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_{\theta} \log \pi(a_t|s_t; \theta) G_t$
6.  **重複**：重複步驟 2-5，直到策略收斂或達到最大迭代次數。

---

## 優點與缺點

### 優點

*   **直接學習策略**：可以直接學習隨機策略，這在某些情況下（例如，當最佳策略是隨機策略時）非常有用。
*   **處理連續動作空間**：策略梯度方法可以自然地擴展到連續動作空間，只需讓策略網路輸出動作的機率分佈的參數（例如均值和方差）。
*   **收斂性**：在理論上，策略梯度方法可以收斂到局部最優策略。

### 缺點

*   **方差大 (High Variance)**：由於蒙特卡洛方法需要完整的軌跡來估計累積獎勵 $G_t$，這會導致梯度估計的方差很大，使得訓練過程不穩定且收斂緩慢。
*   **效率低下**：每次更新都需要一個完整的軌跡，這在環境複雜或軌跡很長時效率低下。
*   **沒有利用價值函數**：REINFORCE 沒有利用價值函數來減少方差，這限制了其性能。

---

## 應用場景

*   **機器人控制**：學習複雜的運動策略。
*   **遊戲 AI**：學習在複雜環境中的決策。
*   **推薦系統**：學習個性化推薦策略。

---

REINFORCE 是策略梯度方法的基礎，理解其直接學習策略和蒙特卡洛估計的原理是掌握基於策略強化學習的關鍵。接下來，我們將探討 Actor-Critic 方法，它結合了策略梯度和價值函數的優點。請持續關注！