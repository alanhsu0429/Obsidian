# 深度循環神經網路 (Deep RNN)

歡迎來到深度循環神經網路 (Deep RNN) 的世界！深度循環神經網路是**循環神經網路 (RNN)** 的一種擴展，它透過**堆疊多個 RNN 層**來增加模型的深度。與傳統的單層 RNN 相比，Deep RNN 能夠學習更複雜、更抽象的序列數據表示，從而在許多複雜的序列任務中取得更好的性能。

---

## 核心概念

*   **堆疊多個 RNN 層 (Stacked RNN Layers)**：Deep RNN 的核心思想是將多個 RNN 層（可以是傳統 RNN、[[長短期記憶網路 (Long Short-Term Memory, LSTM)]] 或 [[門控循環單元 (Gated Recurrent Unit, GRU)]]）堆疊在一起。每個 RNN 層的輸出作為下一層的輸入。
*   **層次化表示學習 (Hierarchical Representation Learning)**：
    *   **底層 RNN**：通常學習序列數據的低級特徵，例如文本中的詞向量、語音中的聲學特徵。
    *   **高層 RNN**：在底層學習到的特徵基礎上，學習更高級、更抽象的特徵，例如文本中的語法結構、語義信息。
*   **信息流動**：
    *   **水平方向**：信息在每個 RNN 層內部沿著時間步傳播（即隱藏狀態的傳遞）。
    *   **垂直方向**：信息從一個 RNN 層的輸出傳遞到下一層的輸入。

---

## Deep RNN 的典型架構

一個 $L$ 層的 Deep RNN 的信息流動可以表示為：

*   **第一層 (Bottom Layer)**：
    $h_t^{(1)} = f(W_{xh}^{(1)}x_t + W_{hh}^{(1)}h_{t-1}^{(1)} + b_h^{(1)})$
*   **中間層 (Intermediate Layers)**：對於 $l = 2, \dots, L$：
    $h_t^{(l)} = f(W_{xh}^{(l)}h_t^{(l-1)} + W_{hh}^{(l)}h_{t-1}^{(l)} + b_h^{(l)})$
*   **輸出層 (Top Layer)**：
    $y_t = W_{hy}h_t^{(L)} + b_y$

其中：
*   $x_t$ 是時間步 $t$ 的輸入。
*   $h_t^{(l)}$ 是時間步 $t$ 的第 $l$ 層的隱藏狀態。
*   $y_t$ 是時間步 $t$ 的輸出。
*   $f$ 是激活函數。

---

## Deep RNN 的優點與缺點

### 優點

*   **學習更複雜的表示**：透過增加深度，模型能夠學習到更複雜、更抽象的序列數據表示，從而提高模型在複雜任務上的性能。
*   **在複雜序列任務中表現出色**：在需要捕捉多層次抽象的任務中，如複雜的自然語言理解、語音識別等，Deep RNN 通常比單層 RNN 表現更好。

### 缺點

*   **訓練難度增加**：隨著層數的增加，模型的參數數量也會增加，訓練難度更大，更容易出現[[過擬合與欠擬合]]。
*   **計算成本高**：訓練深度模型需要更多的計算資源和時間。
*   **梯度問題更嚴重**：雖然 LSTM 和 GRU 解決了單層 RNN 的梯度消失問題，但在非常深的 RNN 中，梯度問題仍然可能再次出現。

---

## 應用場景

*   **高級自然語言處理**：複雜的語義理解、文本摘要、問答系統。
*   **大規模語音識別**：處理複雜的語音信號。
*   **機器翻譯**：處理長而複雜的句子。

---

深度循環神經網路是處理複雜序列數據的強大工具，它透過增加模型的深度來學習更豐富的層次化表示。理解其堆疊結構和信息流動的原理是掌握它的關鍵。至此，我們已經涵蓋了循環神經網路及其主要變體。接下來，我們將探討 Transformer 模型。請持續關注！
